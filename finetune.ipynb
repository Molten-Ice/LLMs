{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface_hub) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/lib/python3/dist-packages (from huggingface_hub) (21.3)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (5.4.1)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (3.3)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, charset-normalizer, requests, tiktoken, huggingface_hub\n",
      "Successfully installed charset-normalizer-3.4.1 huggingface_hub-0.27.0 regex-2024.11.6 requests-2.32.3 tiktoken-0.8.0 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -i C:\\Users\\James\\.ssh\\id_ed25519 ubuntu@209.20.159.87:/home/ubuntu/llm/finetune.ipynb C:\\Users\\James\\git\\LLMs\\finetune.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_openai_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def generate(self, tokens, num_return_sequences=1, max_length=64):\n",
    "        self.eval()\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "        xgen = tokens.to(device)\n",
    "        while xgen.size(1) < max_length:\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "                logits = logits[:, -1, :] # (B, vocab_size)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "                ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "                xgen = torch.cat((xgen, xcol), dim=1)\n",
    "        return xgen[:, :max_length]\n",
    "\n",
    "    def prompt(self, prompt, max_length=64):\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        tokens = enc.encode(prompt)\n",
    "        xgen = self.generate(tokens, num_return_sequences=1, max_length=max_length)\n",
    "        tokens = xgen[0].tolist()\n",
    "        decoded = enc.decode(tokens)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"/home/ubuntu/llm/build-nanogpt/log/model_19072.pt\",\n",
    "#     path_in_repo=\"gpt-v1.pt\",\n",
    "#     repo_id= \"molten-ice/gpt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import huggingface_hub\n",
    "\n",
    "api = huggingface_hub.HfApi()\n",
    "with open('hugging_apikey.json', 'r') as f:\n",
    "    api_key = json.load(f)['api_key']\n",
    "\n",
    "huggingface_hub.login(token=api_key)\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_path = huggingface_hub.hf_hub_download(\n",
    "    repo_id=\"Molten-Ice/gpt\", # https://huggingface.co/Molten-Ice/gpt/tree/main\n",
    "    filename=\"gpt-v1.pt\",\n",
    "    local_dir='models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2831/4262811781.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 50304\n",
    "model = GPT(GPTConfig(vocab_size=vocab_size))\n",
    "model.to(device)\n",
    "print(f'device: {device}')\n",
    "\n",
    "model_path = 'models/gpt-v1.pt'\n",
    "loaded_checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(loaded_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, I'm a language model, meaning I'm a model of models, that is, a system of ways that language, like systems or structures, come together across the language and help to make certain the same rules apply to different ways. So, I'd like to say language models are useful, I think,\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "model.prompt(\"Hello, I'm a language model,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-25 11:12:03--  https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22773992 (22M) [text/plain]\n",
      "Saving to: ‘alpaca_data.json.1’\n",
      "\n",
      "alpaca_data.json.1  100%[===================>]  21.72M  2.73MB/s    in 7.6s    \n",
      "\n",
      "2024-12-25 11:12:11 (2.87 MB/s) - ‘alpaca_data.json.1’ saved [22773992/22773992]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('alpaca_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "base_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "all_instructions = [base_prompt.format(instruction=f'{d[\"instruction\"]}', output=d['output']) for d in data if not d['input']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "The three primary colors are red, blue, and yellow.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Response:\n",
      "An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How can we reduce air pollution?\n",
      "\n",
      "### Response:\n",
      "There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe a time when you had to make a difficult decision.\n",
      "\n",
      "### Response:\n",
      "I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for instruction in all_instructions[:5]:\n",
    "    print(instruction)\n",
    "    print('-'*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_tokens): 3,525,454\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# init the tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
    "def tokenize(text):\n",
    "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
    "    tokens = [eot] # the special <|endoftext|> token delimits all documents\n",
    "    tokens.extend(enc.encode_ordinary(text))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "    return tokens_np_uint16\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "all_tokens = []\n",
    "for instruction in all_instructions:\n",
    "    tokens = tokenize(instruction)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "print(f'len(all_tokens): {len(all_tokens):,}')\n",
    "np.save('alpaca_tokens.npy', all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class AlpacaDataLoader:\n",
    "    def __init__(self, B, T, split='train'):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.current_position = 0\n",
    "\n",
    "        assert split in {'train', 'val'}\n",
    "        self.tokens = load_tokens('alpaca_tokens.npy')\n",
    "        if split == 'train':\n",
    "            self.tokens = self.tokens[:int(0.9 * len(self.tokens))]\n",
    "        else:\n",
    "            self.tokens = self.tokens[int(0.9 * len(self.tokens)):]\n",
    "\n",
    "        batches = len(self.tokens) // (B * T)\n",
    "        print(f'[{split}] {len(self.tokens):,} tokens | {batches:,} batches | {self.tokens.tolist().count(50256):,} eot tokens')\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        self.current_position += B * T\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2831/2931923067.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 50304\n",
    "model = GPT(GPTConfig(vocab_size=vocab_size))\n",
    "model.to(device)\n",
    "print(f'device: {device}')\n",
    "\n",
    "model_path = 'models/gpt-v1.pt'\n",
    "loaded_checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(loaded_checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save(model, val_loader, step):\n",
    "    val_loss_steps = 40\n",
    "    val_loader.reset()\n",
    "    with torch.no_grad():\n",
    "        val_loss_accum = 0.0\n",
    "        for _ in range(val_loss_steps):\n",
    "            x, y = val_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "            loss = loss / val_loss_steps\n",
    "            val_loss_accum += loss.detach()\n",
    "    print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "\n",
    "    # Save model to file\n",
    "    # with open(log_file, \"a\") as f:\n",
    "    #     f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "    # if step > 0 and (step % 5000 == 0 or last_step):\n",
    "    #     # optionally write model checkpoints\n",
    "    #     checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
    "    #     checkpoint = {\n",
    "    #         'model': raw_model.state_dict(),\n",
    "    #         'config': raw_model.config,\n",
    "    #         'step': step,\n",
    "    #         'val_loss': val_loss_accum.item()\n",
    "    #     }\n",
    "    #     # you might also want to add optimizer.state_dict() and\n",
    "    #     # rng seeds etc., if you wanted to more exactly resume training\n",
    "    #     torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # Generate example outputs based on instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715\n",
    "max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] 3,172,908 tokens | 387 batches | 28,210 eot tokens\n",
      "[val] 352,546 tokens | 43 batches | 3,113 eot tokens\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step     0 | loss: 2.927885 | lr 8.3916e-07 | norm: 13.1722 | dt: 393.85ms | tok/sec: 20799.94\n",
      "step     1 | loss: 2.991701 | lr 1.6783e-06 | norm: 15.1542 | dt: 56.20ms | tok/sec: 145752.69\n",
      "step     2 | loss: 2.858903 | lr 2.5175e-06 | norm: 15.9752 | dt: 50.09ms | tok/sec: 163532.14\n",
      "step     3 | loss: 2.753336 | lr 3.3566e-06 | norm: 11.5365 | dt: 49.16ms | tok/sec: 166645.19\n",
      "step     4 | loss: 2.727894 | lr 4.1958e-06 | norm: 9.8177 | dt: 53.44ms | tok/sec: 153300.67\n",
      "step     5 | loss: 2.480538 | lr 5.0350e-06 | norm: 7.4541 | dt: 51.58ms | tok/sec: 158830.21\n",
      "step     6 | loss: 2.649940 | lr 5.8741e-06 | norm: 6.0123 | dt: 51.60ms | tok/sec: 158758.29\n",
      "step     7 | loss: 2.408336 | lr 6.7133e-06 | norm: 4.9929 | dt: 51.50ms | tok/sec: 159072.86\n",
      "step     8 | loss: 2.251108 | lr 7.5524e-06 | norm: 4.2456 | dt: 53.41ms | tok/sec: 153386.21\n",
      "step     9 | loss: 2.344805 | lr 8.3916e-06 | norm: 4.6351 | dt: 51.39ms | tok/sec: 159418.27\n",
      "step    10 | loss: 2.222946 | lr 9.2308e-06 | norm: 5.3395 | dt: 51.49ms | tok/sec: 159089.80\n",
      "step    11 | loss: 2.201875 | lr 1.0070e-05 | norm: 4.6013 | dt: 51.24ms | tok/sec: 159886.36\n",
      "step    12 | loss: 2.167643 | lr 1.0909e-05 | norm: 4.5570 | dt: 53.14ms | tok/sec: 154159.73\n",
      "step    13 | loss: 2.057890 | lr 1.1748e-05 | norm: 3.8818 | dt: 51.82ms | tok/sec: 158089.92\n",
      "step    14 | loss: 1.993030 | lr 1.2587e-05 | norm: 3.3544 | dt: 51.34ms | tok/sec: 159578.93\n",
      "step    15 | loss: 1.998452 | lr 1.3427e-05 | norm: 3.4479 | dt: 51.47ms | tok/sec: 159161.29\n",
      "step    16 | loss: 2.066875 | lr 1.4266e-05 | norm: 3.1460 | dt: 53.45ms | tok/sec: 153259.64\n",
      "step    17 | loss: 1.944835 | lr 1.5105e-05 | norm: 2.6442 | dt: 51.50ms | tok/sec: 159058.87\n",
      "step    18 | loss: 1.963856 | lr 1.5944e-05 | norm: 2.1964 | dt: 52.20ms | tok/sec: 156928.18\n",
      "step    19 | loss: 1.917551 | lr 1.6783e-05 | norm: 2.2336 | dt: 51.28ms | tok/sec: 159745.87\n",
      "step    20 | loss: 1.911189 | lr 1.7622e-05 | norm: 2.1524 | dt: 53.37ms | tok/sec: 153491.73\n",
      "step    21 | loss: 2.087559 | lr 1.8462e-05 | norm: 2.5061 | dt: 51.53ms | tok/sec: 158987.48\n",
      "step    22 | loss: 1.746779 | lr 1.9301e-05 | norm: 2.0420 | dt: 51.32ms | tok/sec: 159624.90\n",
      "step    23 | loss: 1.843210 | lr 2.0140e-05 | norm: 1.9153 | dt: 51.30ms | tok/sec: 159698.35\n",
      "step    24 | loss: 1.796100 | lr 2.0979e-05 | norm: 1.8506 | dt: 53.29ms | tok/sec: 153725.22\n",
      "step    25 | loss: 1.923507 | lr 2.1818e-05 | norm: 1.7993 | dt: 51.39ms | tok/sec: 159407.91\n",
      "step    26 | loss: 1.878333 | lr 2.2657e-05 | norm: 1.7274 | dt: 51.33ms | tok/sec: 159591.54\n",
      "step    27 | loss: 1.651551 | lr 2.3497e-05 | norm: 1.5734 | dt: 53.67ms | tok/sec: 152638.04\n",
      "step    28 | loss: 1.815882 | lr 2.4336e-05 | norm: 1.6767 | dt: 53.17ms | tok/sec: 154072.64\n",
      "step    29 | loss: 1.816509 | lr 2.5175e-05 | norm: 1.6619 | dt: 51.73ms | tok/sec: 158348.56\n",
      "step    30 | loss: 1.849779 | lr 2.6014e-05 | norm: 1.6132 | dt: 51.51ms | tok/sec: 159022.06\n",
      "step    31 | loss: 1.750799 | lr 2.6853e-05 | norm: 1.4631 | dt: 51.55ms | tok/sec: 158907.33\n",
      "step    32 | loss: 1.495692 | lr 2.7692e-05 | norm: 1.4058 | dt: 53.93ms | tok/sec: 151887.73\n",
      "step    33 | loss: 1.736379 | lr 2.8531e-05 | norm: 1.3871 | dt: 52.13ms | tok/sec: 157158.58\n",
      "step    34 | loss: 1.801126 | lr 2.9371e-05 | norm: 1.5075 | dt: 51.53ms | tok/sec: 158972.03\n",
      "step    35 | loss: 1.680202 | lr 3.0210e-05 | norm: 1.5167 | dt: 51.44ms | tok/sec: 159253.50\n",
      "step    36 | loss: 1.657010 | lr 3.1049e-05 | norm: 1.4937 | dt: 53.64ms | tok/sec: 152731.67\n",
      "step    37 | loss: 1.720152 | lr 3.1888e-05 | norm: 1.4315 | dt: 51.71ms | tok/sec: 158416.46\n",
      "step    38 | loss: 1.794341 | lr 3.2727e-05 | norm: 1.9936 | dt: 51.62ms | tok/sec: 158707.69\n",
      "step    39 | loss: 1.760209 | lr 3.3566e-05 | norm: 1.6794 | dt: 51.49ms | tok/sec: 159110.43\n",
      "step    40 | loss: 1.696961 | lr 3.4406e-05 | norm: 1.5337 | dt: 53.45ms | tok/sec: 153267.16\n",
      "step    41 | loss: 1.669593 | lr 3.5245e-05 | norm: 1.4932 | dt: 51.75ms | tok/sec: 158313.54\n",
      "step    42 | loss: 1.665533 | lr 3.6084e-05 | norm: 1.3706 | dt: 51.34ms | tok/sec: 159551.52\n",
      "step    43 | loss: 1.685999 | lr 3.6923e-05 | norm: 1.9137 | dt: 51.71ms | tok/sec: 158435.45\n",
      "step    44 | loss: 1.900259 | lr 3.7762e-05 | norm: 1.5280 | dt: 53.44ms | tok/sec: 153284.25\n",
      "step    45 | loss: 1.744205 | lr 3.8601e-05 | norm: 1.4579 | dt: 53.73ms | tok/sec: 152462.62\n",
      "step    46 | loss: 1.659970 | lr 3.9441e-05 | norm: 1.8769 | dt: 51.70ms | tok/sec: 158462.49\n",
      "step    47 | loss: 1.667369 | lr 4.0280e-05 | norm: 1.3725 | dt: 51.37ms | tok/sec: 159455.26\n",
      "step    48 | loss: 1.664582 | lr 4.1119e-05 | norm: 1.8528 | dt: 53.19ms | tok/sec: 154013.24\n",
      "step    49 | loss: 1.698289 | lr 4.1958e-05 | norm: 1.5164 | dt: 51.60ms | tok/sec: 158755.35\n"
     ]
    }
   ],
   "source": [
    "max_steps = 1200\n",
    "\n",
    "train_loader = AlpacaDataLoader(B, T, split='train')\n",
    "val_loader = AlpacaDataLoader(B, T, split='val')\n",
    "\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
    "\n",
    "\n",
    "for step in range(50): # ~3 epochs of 32k example instructions\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    tokens_processed = train_loader.B * train_loader.T\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "   \n",
    "    print(f\"step {step:5d} | loss: {loss.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
