{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -i C:\\Users\\James\\.ssh\\id_ed25519 ubuntu@209.20.156.88:/home/ubuntu/llm/finetune.ipynb C:\\Users\\James\\git\\LLMs\\finetune.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_openai_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def generate(self, tokens, num_return_sequences=1, max_length=64):\n",
    "        self.eval()\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "        xgen = tokens.to(device)\n",
    "        while xgen.size(1) < max_length:\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "                logits = logits[:, -1, :] # (B, vocab_size)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "                ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "                xgen = torch.cat((xgen, xcol), dim=1)\n",
    "        return xgen[:, :max_length]\n",
    "\n",
    "    def prompt(self, prompt, max_length=64):\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        tokens = enc.encode(prompt)\n",
    "        xgen = self.generate(tokens, num_return_sequences=1, max_length=max_length)\n",
    "        tokens = xgen[0].tolist()\n",
    "        decoded = enc.decode(tokens)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"/home/ubuntu/llm/build-nanogpt/log/model_19072.pt\",\n",
    "#     path_in_repo=\"gpt-v1.pt\",\n",
    "#     repo_id= \"molten-ice/gpt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import huggingface_hub\n",
    "\n",
    "api = huggingface_hub.HfApi()\n",
    "with open('hugging_apikey.json', 'r') as f:\n",
    "    api_key = json.load(f)['api_key']\n",
    "\n",
    "huggingface_hub.login(token=api_key)\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_path = huggingface_hub.hf_hub_download(\n",
    "    repo_id=\"Molten-Ice/gpt\", # https://huggingface.co/Molten-Ice/gpt/tree/main\n",
    "    filename=\"gpt-v1.pt\",\n",
    "    local_dir='models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('alpaca_data.json', 'r') as f:\n",
    "    alpaca_data = json.load(f)\n",
    "\n",
    "BASE_PROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "all_instructions = [BASE_PROMPT.format(instruction=f'{d[\"instruction\"]}', output=d['output']) for d in alpaca_data if not d['input']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instruction in all_instructions[:5]:\n",
    "    print(instruction)\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import numpy as np\n",
    "# init the tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
    "def tokenize(text):\n",
    "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
    "    tokens = [eot] # the special <|endoftext|> token delimits all documents\n",
    "    tokens.extend(enc.encode_ordinary(text))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "    return tokens_np_uint16\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "all_tokens = []\n",
    "for instruction in all_instructions:\n",
    "    tokens = tokenize(instruction)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "print(f'len(all_tokens): {len(all_tokens):,}')\n",
    "np.save('alpaca_tokens.npy', all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 50256}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc._special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class AlpacaDataLoader:\n",
    "    def __init__(self, B, T, split='train'):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.current_position = 0\n",
    "        self.iterated = False\n",
    "\n",
    "        assert split in {'train', 'val'}\n",
    "        self.tokens = load_tokens('alpaca_tokens.npy')\n",
    "        if split == 'train':\n",
    "            self.tokens = self.tokens[int(0.1 * len(self.tokens)):]\n",
    "        else:\n",
    "            self.tokens = self.tokens[:int(0.1 * len(self.tokens))]\n",
    "\n",
    "        batches = len(self.tokens) // (B * T)\n",
    "        print(f'[{split}] {len(self.tokens):,} tokens | {batches:,} batches | {self.tokens.tolist().count(50256):,} eot tokens')\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = 0\n",
    "        self.iterated = False\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        self.current_position += B * T\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "            self.iterated = True\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715\n",
    "max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "def evaluate_and_generate(model, val_loader, step):\n",
    "    print(f\"Evaluating model at step {step}\")\n",
    "    val_loader.reset()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        while not val_loader.iterated:\n",
    "            x, y = val_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "            val_losses.append(loss.detach())\n",
    "        val_loss = torch.mean(torch.stack(val_losses)).item()\n",
    "    print(f\"validation loss (/ {len(val_losses)}): {val_loss:.4f} \")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"{step} val {val_loss:.4f}\\n\")\n",
    "\n",
    "    outputs = \"\"\n",
    "    for instruction in alpaca_data[:3]:\n",
    "        prompt = BASE_PROMPT.format(instruction=f'{instruction[\"instruction\"]}', output='')\n",
    "        target = instruction['output']\n",
    "        output = model.prompt(prompt)\n",
    "        outputs += '-' *30\n",
    "        outputs += f'\\n{output}\\n'\n",
    "        outputs += f'-- target --\\n{target}\\n'\n",
    "       \n",
    "    print(outputs)\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(outputs)\n",
    "    model.train()\n",
    "    return val_loss, outputs\n",
    "\n",
    "def save_checkpoint(model, step, val_loss, outputs):\n",
    "    model.eval()\n",
    "    # optionally write model checkpoints\n",
    "    checkpoint_path = os.path.join(log_dir, f\"model_{step:04d}.pt\")\n",
    "    print(f'Saving model checkpoint to {checkpoint_path}')\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'config': model.config,\n",
    "        'step': step,\n",
    "        'val_loss': val_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    model.train()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7013/1952856989.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 50304\n",
    "model = GPT(GPTConfig(vocab_size=vocab_size))\n",
    "\n",
    "model_path = 'models/gpt-v1.pt'\n",
    "loaded_checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(loaded_checkpoint['model'])\n",
    "\n",
    "model.to(device)\n",
    "print(f'device: {device}')\n",
    "\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_steps: 387, epoch_steps: 387\n",
      "[train] 3,172,909 tokens | 387 batches | 28,121 eot tokens\n",
      "[val] 352,545 tokens | 43 batches | 3,202 eot tokens\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "Evaluating model at step 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss (/ 43): 2.9809 \n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "Give one advice to remember what to do when dealing with health problems.\n",
      "### Response: Be sure to identify things that you care about\n",
      "-- target --\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "What is the response in the document?\n",
      "### Response:\n",
      "Why is the document written in blue?\n",
      "### Response:\n",
      "Why\n",
      "-- target --\n",
      "The three primary colors are red, blue, and yellow.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Response:\n",
      "Write a response addressing a question and explain a key point by demonstrating a critical review of important points, supporting evidence, and a conclusion\n",
      "-- target --\n",
      "An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n",
      "\n",
      "step    0 | loss: 2.838129 | lr 8.3916e-07 | norm: 12.2341 | dt: 895.26ms | tok/sec: 9150.44\n",
      "step    1 | loss: 3.001495 | lr 1.6783e-06 | norm: 11.4475 | dt: 39.03ms | tok/sec: 209909.94\n",
      "step    2 | loss: 2.887640 | lr 2.5175e-06 | norm: 13.6993 | dt: 38.83ms | tok/sec: 210990.10\n",
      "step    3 | loss: 2.688412 | lr 3.3566e-06 | norm: 12.0946 | dt: 40.09ms | tok/sec: 204349.53\n",
      "step    4 | loss: 2.699029 | lr 4.1958e-06 | norm: 10.9466 | dt: 41.12ms | tok/sec: 199234.24\n",
      "step    5 | loss: 2.504813 | lr 5.0350e-06 | norm: 6.9440 | dt: 44.55ms | tok/sec: 183862.85\n",
      "step    6 | loss: 2.508482 | lr 5.8741e-06 | norm: 6.0361 | dt: 42.88ms | tok/sec: 191048.77\n",
      "step    7 | loss: 2.451165 | lr 6.7133e-06 | norm: 4.5208 | dt: 44.06ms | tok/sec: 185934.35\n",
      "step    8 | loss: 2.369239 | lr 7.5524e-06 | norm: 3.7501 | dt: 44.90ms | tok/sec: 182459.82\n",
      "step    9 | loss: 2.449913 | lr 8.3916e-06 | norm: 4.3135 | dt: 48.53ms | tok/sec: 168817.38\n",
      "step   10 | loss: 2.326025 | lr 9.2308e-06 | norm: 4.6719 | dt: 49.42ms | tok/sec: 165770.45\n",
      "step   11 | loss: 2.337903 | lr 1.0070e-05 | norm: 4.7162 | dt: 49.33ms | tok/sec: 166053.25\n",
      "step   12 | loss: 2.245617 | lr 1.0909e-05 | norm: 3.9694 | dt: 48.88ms | tok/sec: 167600.30\n",
      "step   13 | loss: 2.172567 | lr 1.1748e-05 | norm: 3.4630 | dt: 49.96ms | tok/sec: 163956.65\n",
      "step   14 | loss: 2.133938 | lr 1.2587e-05 | norm: 3.4808 | dt: 49.95ms | tok/sec: 163990.29\n",
      "step   15 | loss: 2.016653 | lr 1.3427e-05 | norm: 2.9405 | dt: 48.95ms | tok/sec: 167348.88\n",
      "step   16 | loss: 1.973636 | lr 1.4266e-05 | norm: 2.8934 | dt: 49.48ms | tok/sec: 165553.20\n",
      "step   17 | loss: 2.063905 | lr 1.5105e-05 | norm: 2.4697 | dt: 50.11ms | tok/sec: 163465.23\n",
      "step   18 | loss: 1.857174 | lr 1.5944e-05 | norm: 2.1933 | dt: 49.34ms | tok/sec: 166043.62\n",
      "step   19 | loss: 1.925234 | lr 1.6783e-05 | norm: 2.0912 | dt: 48.93ms | tok/sec: 167426.35\n",
      "step   20 | loss: 1.917370 | lr 1.7622e-05 | norm: 2.0683 | dt: 48.84ms | tok/sec: 167726.30\n",
      "step   21 | loss: 1.810780 | lr 1.8462e-05 | norm: 2.0203 | dt: 49.72ms | tok/sec: 164779.10\n",
      "step   22 | loss: 1.835075 | lr 1.9301e-05 | norm: 2.1262 | dt: 49.00ms | tok/sec: 167194.16\n",
      "step   23 | loss: 1.918188 | lr 2.0140e-05 | norm: 1.9530 | dt: 49.30ms | tok/sec: 166167.28\n",
      "step   24 | loss: 1.867754 | lr 2.0979e-05 | norm: 2.0834 | dt: 48.96ms | tok/sec: 167311.40\n",
      "step   25 | loss: 1.907447 | lr 2.1818e-05 | norm: 1.8664 | dt: 50.00ms | tok/sec: 163833.91\n",
      "step   26 | loss: 1.858327 | lr 2.2657e-05 | norm: 1.9226 | dt: 49.20ms | tok/sec: 166499.03\n",
      "step   27 | loss: 1.799944 | lr 2.3497e-05 | norm: 1.6985 | dt: 49.72ms | tok/sec: 164752.24\n",
      "step   28 | loss: 1.822268 | lr 2.4336e-05 | norm: 1.6638 | dt: 49.10ms | tok/sec: 166839.39\n",
      "step   29 | loss: 1.619580 | lr 2.5175e-05 | norm: 1.6071 | dt: 50.17ms | tok/sec: 163297.43\n",
      "step   30 | loss: 1.911034 | lr 2.6014e-05 | norm: 1.6286 | dt: 49.32ms | tok/sec: 166107.03\n",
      "step   31 | loss: 1.880684 | lr 2.6853e-05 | norm: 2.1201 | dt: 48.86ms | tok/sec: 167658.37\n",
      "step   32 | loss: 1.730796 | lr 2.7692e-05 | norm: 1.4492 | dt: 48.68ms | tok/sec: 168275.01\n",
      "step   33 | loss: 1.843472 | lr 2.8531e-05 | norm: 1.5667 | dt: 50.33ms | tok/sec: 162759.05\n",
      "step   34 | loss: 1.805776 | lr 2.9371e-05 | norm: 1.5173 | dt: 48.96ms | tok/sec: 167315.47\n",
      "step   35 | loss: 1.642221 | lr 3.0210e-05 | norm: 1.3669 | dt: 48.80ms | tok/sec: 167878.72\n",
      "step   36 | loss: 1.675743 | lr 3.1049e-05 | norm: 1.5832 | dt: 48.93ms | tok/sec: 167416.56\n",
      "step   37 | loss: 1.674790 | lr 3.1888e-05 | norm: 1.6296 | dt: 50.86ms | tok/sec: 161053.98\n",
      "step   38 | loss: 1.836806 | lr 3.2727e-05 | norm: 1.4168 | dt: 49.57ms | tok/sec: 165256.20\n",
      "step   39 | loss: 1.836749 | lr 3.3566e-05 | norm: 1.6236 | dt: 49.23ms | tok/sec: 166416.77\n",
      "step   40 | loss: 1.789956 | lr 3.4406e-05 | norm: 1.7143 | dt: 48.76ms | tok/sec: 168017.46\n",
      "step   41 | loss: 1.753541 | lr 3.5245e-05 | norm: 1.4661 | dt: 50.83ms | tok/sec: 161152.93\n",
      "step   42 | loss: 1.769573 | lr 3.6084e-05 | norm: 1.4505 | dt: 49.56ms | tok/sec: 165299.13\n",
      "step   43 | loss: 1.769413 | lr 3.6923e-05 | norm: 1.4568 | dt: 49.30ms | tok/sec: 166159.25\n",
      "step   44 | loss: 1.949114 | lr 3.7762e-05 | norm: 1.5538 | dt: 49.12ms | tok/sec: 166769.75\n",
      "step   45 | loss: 1.845279 | lr 3.8601e-05 | norm: 2.2349 | dt: 50.35ms | tok/sec: 162686.61\n",
      "step   46 | loss: 1.648637 | lr 3.9441e-05 | norm: 1.4423 | dt: 49.30ms | tok/sec: 166158.44\n",
      "step   47 | loss: 1.665854 | lr 4.0280e-05 | norm: 1.5897 | dt: 48.81ms | tok/sec: 167825.42\n",
      "step   48 | loss: 1.831940 | lr 4.1119e-05 | norm: 1.5085 | dt: 49.15ms | tok/sec: 166670.25\n",
      "step   49 | loss: 1.737169 | lr 4.1958e-05 | norm: 1.4820 | dt: 50.26ms | tok/sec: 163004.59\n",
      "step   50 | loss: 1.750102 | lr 4.2797e-05 | norm: 1.5187 | dt: 49.41ms | tok/sec: 165782.45\n",
      "step   51 | loss: 1.754158 | lr 4.3636e-05 | norm: 1.4129 | dt: 49.31ms | tok/sec: 166124.70\n",
      "step   52 | loss: 1.667332 | lr 4.4476e-05 | norm: 1.4217 | dt: 48.98ms | tok/sec: 167262.53\n",
      "step   53 | loss: 1.694459 | lr 4.5315e-05 | norm: 1.5829 | dt: 50.45ms | tok/sec: 162375.24\n",
      "step   54 | loss: 1.742441 | lr 4.6154e-05 | norm: 1.4713 | dt: 49.21ms | tok/sec: 166469.99\n",
      "step   55 | loss: 1.666757 | lr 4.6993e-05 | norm: 1.3656 | dt: 48.86ms | tok/sec: 167679.64\n",
      "step   56 | loss: 1.641643 | lr 4.7832e-05 | norm: 1.4542 | dt: 48.99ms | tok/sec: 167201.49\n",
      "step   57 | loss: 1.618876 | lr 4.8671e-05 | norm: 1.3800 | dt: 49.77ms | tok/sec: 164607.80\n",
      "step   58 | loss: 1.806981 | lr 4.9510e-05 | norm: 1.4910 | dt: 49.23ms | tok/sec: 166411.94\n",
      "step   59 | loss: 1.786072 | lr 5.0350e-05 | norm: 1.5060 | dt: 49.00ms | tok/sec: 167186.84\n",
      "step   60 | loss: 1.631161 | lr 5.1189e-05 | norm: 1.5018 | dt: 49.24ms | tok/sec: 166360.37\n",
      "step   61 | loss: 1.639390 | lr 5.2028e-05 | norm: 1.4448 | dt: 49.86ms | tok/sec: 164313.40\n",
      "step   62 | loss: 1.674707 | lr 5.2867e-05 | norm: 1.4218 | dt: 49.31ms | tok/sec: 166143.18\n",
      "step   63 | loss: 1.856936 | lr 5.3706e-05 | norm: 1.4190 | dt: 49.16ms | tok/sec: 166641.96\n",
      "step   64 | loss: 1.751228 | lr 5.4545e-05 | norm: 1.4051 | dt: 48.99ms | tok/sec: 167216.13\n",
      "step   65 | loss: 1.599596 | lr 5.5385e-05 | norm: 1.4990 | dt: 50.50ms | tok/sec: 162219.62\n",
      "step   66 | loss: 1.685075 | lr 5.6224e-05 | norm: 1.4032 | dt: 48.99ms | tok/sec: 167215.32\n",
      "step   67 | loss: 1.692953 | lr 5.7063e-05 | norm: 1.4240 | dt: 48.92ms | tok/sec: 167456.54\n",
      "step   68 | loss: 1.749111 | lr 5.7902e-05 | norm: 1.3913 | dt: 48.93ms | tok/sec: 167436.14\n",
      "step   69 | loss: 1.656789 | lr 5.8741e-05 | norm: 1.8884 | dt: 50.26ms | tok/sec: 162993.77\n",
      "step   70 | loss: 1.627421 | lr 5.9580e-05 | norm: 1.4493 | dt: 49.23ms | tok/sec: 166411.13\n",
      "step   71 | loss: 1.521315 | lr 6.0420e-05 | norm: 1.3674 | dt: 52.22ms | tok/sec: 156862.98\n",
      "step   72 | loss: 1.566280 | lr 6.1259e-05 | norm: 1.4580 | dt: 49.03ms | tok/sec: 167081.16\n",
      "step   73 | loss: 1.802026 | lr 6.2098e-05 | norm: 1.4926 | dt: 51.28ms | tok/sec: 159759.24\n",
      "step   74 | loss: 1.706181 | lr 6.2937e-05 | norm: 1.3883 | dt: 49.79ms | tok/sec: 164526.62\n",
      "step   75 | loss: 1.687160 | lr 6.3776e-05 | norm: 1.4777 | dt: 48.68ms | tok/sec: 168271.72\n",
      "step   76 | loss: 1.665365 | lr 6.4615e-05 | norm: 1.4277 | dt: 48.87ms | tok/sec: 167638.74\n",
      "step   77 | loss: 1.822671 | lr 6.5455e-05 | norm: 1.5802 | dt: 50.14ms | tok/sec: 163379.73\n",
      "step   78 | loss: 1.575281 | lr 6.6294e-05 | norm: 1.3546 | dt: 49.16ms | tok/sec: 166642.76\n",
      "step   79 | loss: 1.818745 | lr 6.7133e-05 | norm: 1.5484 | dt: 48.95ms | tok/sec: 167365.19\n",
      "step   80 | loss: 1.672812 | lr 6.7972e-05 | norm: 1.4354 | dt: 49.05ms | tok/sec: 167027.55\n",
      "step   81 | loss: 1.883381 | lr 6.8811e-05 | norm: 1.5539 | dt: 50.06ms | tok/sec: 163650.53\n",
      "step   82 | loss: 1.675997 | lr 6.9650e-05 | norm: 1.3978 | dt: 49.26ms | tok/sec: 166295.96\n",
      "step   83 | loss: 1.761818 | lr 7.0490e-05 | norm: 1.4253 | dt: 48.95ms | tok/sec: 167355.41\n",
      "step   84 | loss: 1.720901 | lr 7.1329e-05 | norm: 1.4556 | dt: 49.24ms | tok/sec: 166364.40\n",
      "step   85 | loss: 1.699644 | lr 7.2168e-05 | norm: 1.4417 | dt: 49.70ms | tok/sec: 164824.16\n",
      "step   86 | loss: 1.782273 | lr 7.3007e-05 | norm: 1.4865 | dt: 49.16ms | tok/sec: 166625.79\n",
      "step   87 | loss: 1.604852 | lr 7.3846e-05 | norm: 1.3467 | dt: 49.30ms | tok/sec: 166160.05\n",
      "step   88 | loss: 1.693845 | lr 7.4685e-05 | norm: 1.4085 | dt: 49.25ms | tok/sec: 166321.72\n",
      "step   89 | loss: 1.613574 | lr 7.5524e-05 | norm: 1.4133 | dt: 50.04ms | tok/sec: 163703.55\n",
      "step   90 | loss: 1.585703 | lr 7.6364e-05 | norm: 1.3387 | dt: 49.80ms | tok/sec: 164510.86\n",
      "step   91 | loss: 1.685171 | lr 7.7203e-05 | norm: 1.4565 | dt: 49.25ms | tok/sec: 166328.16\n",
      "step   92 | loss: 1.650924 | lr 7.8042e-05 | norm: 1.4441 | dt: 49.16ms | tok/sec: 166630.64\n",
      "step   93 | loss: 1.598966 | lr 7.8881e-05 | norm: 1.3955 | dt: 50.77ms | tok/sec: 161370.15\n",
      "step   94 | loss: 1.583611 | lr 7.9720e-05 | norm: 1.3782 | dt: 49.08ms | tok/sec: 166917.20\n",
      "step   95 | loss: 1.726561 | lr 8.0559e-05 | norm: 1.4593 | dt: 48.78ms | tok/sec: 167944.37\n",
      "step   96 | loss: 1.646230 | lr 8.1399e-05 | norm: 1.5063 | dt: 49.17ms | tok/sec: 166617.71\n",
      "step   97 | loss: 1.479555 | lr 8.2238e-05 | norm: 1.3932 | dt: 50.07ms | tok/sec: 163602.22\n",
      "step   98 | loss: 1.607038 | lr 8.3077e-05 | norm: 1.3990 | dt: 49.30ms | tok/sec: 166169.69\n",
      "step   99 | loss: 1.650094 | lr 8.3916e-05 | norm: 1.4189 | dt: 48.80ms | tok/sec: 167857.40\n",
      "Evaluating model at step 100\n",
      "validation loss (/ 43): 1.6440 \n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1. Consume plenty of fruits and vegetables every 2-3 times. \n",
      "2. Cut down on the amount of caffeine and\n",
      "-- target --\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "The three primary colors are red, green and orange, and have distinct differences in tone, luminosity, and intensity. Red is soft\n",
      "-- target --\n",
      "The three primary colors are red, blue, and yellow.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Response:\n",
      "An atom is a repeating unit made up of 3...\n",
      "- Nucleus 1\n",
      "- Helium 2\n",
      "- Neon 0\n",
      "-- target --\n",
      "An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n",
      "\n",
      "step  100 | loss: 1.764719 | lr 8.4755e-05 | norm: 1.4492 | dt: 897.03ms | tok/sec: 9132.37\n",
      "step  101 | loss: 1.613518 | lr 8.5594e-05 | norm: 1.4374 | dt: 38.19ms | tok/sec: 214505.71\n",
      "step  102 | loss: 1.648544 | lr 8.6434e-05 | norm: 1.3443 | dt: 40.05ms | tok/sec: 204529.56\n",
      "step  103 | loss: 1.814443 | lr 8.7273e-05 | norm: 1.6242 | dt: 43.70ms | tok/sec: 187472.31\n",
      "step  104 | loss: 1.802598 | lr 8.8112e-05 | norm: 1.4217 | dt: 47.09ms | tok/sec: 173982.17\n",
      "step  105 | loss: 1.780554 | lr 8.8951e-05 | norm: 1.4645 | dt: 47.39ms | tok/sec: 172858.31\n",
      "step  106 | loss: 1.677903 | lr 8.9790e-05 | norm: 1.4004 | dt: 48.76ms | tok/sec: 168015.82\n",
      "step  107 | loss: 1.668009 | lr 9.0629e-05 | norm: 1.3986 | dt: 49.19ms | tok/sec: 166538.57\n",
      "step  108 | loss: 1.838162 | lr 9.1469e-05 | norm: 1.4434 | dt: 49.26ms | tok/sec: 166303.20\n",
      "step  109 | loss: 1.696046 | lr 9.2308e-05 | norm: 1.5054 | dt: 49.71ms | tok/sec: 164807.56\n",
      "step  110 | loss: 1.679105 | lr 9.3147e-05 | norm: 1.4453 | dt: 49.15ms | tok/sec: 166677.52\n",
      "step  111 | loss: 1.629519 | lr 9.3986e-05 | norm: 1.7137 | dt: 49.65ms | tok/sec: 164984.03\n",
      "step  112 | loss: 1.797410 | lr 9.4825e-05 | norm: 1.4653 | dt: 50.58ms | tok/sec: 161957.35\n",
      "step  113 | loss: 1.726146 | lr 9.5664e-05 | norm: 1.6278 | dt: 49.23ms | tok/sec: 166402.27\n",
      "step  114 | loss: 1.758256 | lr 9.6503e-05 | norm: 1.4874 | dt: 48.77ms | tok/sec: 167961.61\n",
      "step  115 | loss: 1.609185 | lr 9.7343e-05 | norm: 1.4706 | dt: 48.70ms | tok/sec: 168227.23\n",
      "step  116 | loss: 1.678886 | lr 9.8182e-05 | norm: 1.5106 | dt: 50.42ms | tok/sec: 162473.52\n",
      "step  117 | loss: 1.750516 | lr 9.9021e-05 | norm: 2.0836 | dt: 49.31ms | tok/sec: 166119.08\n",
      "step  118 | loss: 1.600166 | lr 9.9860e-05 | norm: 1.4386 | dt: 51.10ms | tok/sec: 160314.56\n",
      "step  119 | loss: 1.625770 | lr 1.0070e-04 | norm: 1.3417 | dt: 48.95ms | tok/sec: 167337.47\n",
      "step  120 | loss: 1.637282 | lr 1.0154e-04 | norm: 1.4322 | dt: 48.54ms | tok/sec: 168761.82\n",
      "step  121 | loss: 1.635184 | lr 1.0238e-04 | norm: 1.5591 | dt: 44.44ms | tok/sec: 184331.38\n",
      "step  122 | loss: 1.763111 | lr 1.0322e-04 | norm: 1.7999 | dt: 46.16ms | tok/sec: 177483.49\n",
      "step  123 | loss: 1.782575 | lr 1.0406e-04 | norm: 1.5195 | dt: 45.90ms | tok/sec: 178468.97\n",
      "step  124 | loss: 1.556302 | lr 1.0490e-04 | norm: 1.3820 | dt: 46.57ms | tok/sec: 175904.30\n",
      "step  125 | loss: 1.667086 | lr 1.0573e-04 | norm: 1.3926 | dt: 48.28ms | tok/sec: 169665.15\n",
      "step  126 | loss: 1.673682 | lr 1.0657e-04 | norm: 1.3981 | dt: 46.95ms | tok/sec: 174478.68\n",
      "step  127 | loss: 1.663892 | lr 1.0741e-04 | norm: 1.4878 | dt: 48.90ms | tok/sec: 167542.28\n",
      "step  128 | loss: 1.654798 | lr 1.0825e-04 | norm: 1.4365 | dt: 49.06ms | tok/sec: 166965.05\n",
      "step  129 | loss: 1.725322 | lr 1.0909e-04 | norm: 1.3980 | dt: 49.99ms | tok/sec: 163866.72\n",
      "step  130 | loss: 1.655460 | lr 1.0993e-04 | norm: 1.4541 | dt: 49.02ms | tok/sec: 167116.10\n",
      "step  131 | loss: 1.664230 | lr 1.1077e-04 | norm: 1.4792 | dt: 48.83ms | tok/sec: 167763.15\n",
      "step  132 | loss: 1.752733 | lr 1.1161e-04 | norm: 1.3805 | dt: 49.77ms | tok/sec: 164588.88\n",
      "step  133 | loss: 1.839895 | lr 1.1245e-04 | norm: 1.5273 | dt: 50.24ms | tok/sec: 163050.23\n",
      "step  134 | loss: 1.770263 | lr 1.1329e-04 | norm: 1.6568 | dt: 49.49ms | tok/sec: 165542.03\n",
      "step  135 | loss: 1.694112 | lr 1.1413e-04 | norm: 1.3186 | dt: 49.22ms | tok/sec: 166428.06\n",
      "step  136 | loss: 1.699986 | lr 1.1497e-04 | norm: 1.4585 | dt: 49.39ms | tok/sec: 165855.27\n",
      "step  137 | loss: 1.850366 | lr 1.1580e-04 | norm: 1.4141 | dt: 50.52ms | tok/sec: 162143.84\n",
      "step  138 | loss: 1.621598 | lr 1.1664e-04 | norm: 1.3650 | dt: 49.49ms | tok/sec: 165526.08\n",
      "step  139 | loss: 1.709367 | lr 1.1748e-04 | norm: 1.3830 | dt: 49.42ms | tok/sec: 165760.05\n",
      "step  140 | loss: 1.718510 | lr 1.1832e-04 | norm: 1.4451 | dt: 48.94ms | tok/sec: 167374.97\n",
      "step  141 | loss: 1.801166 | lr 1.1916e-04 | norm: 1.4657 | dt: 50.44ms | tok/sec: 162407.48\n",
      "step  142 | loss: 1.684840 | lr 1.2000e-04 | norm: 1.3365 | dt: 49.19ms | tok/sec: 166532.11\n",
      "step  143 | loss: 1.660603 | lr 1.2084e-04 | norm: 1.4640 | dt: 49.14ms | tok/sec: 166711.49\n",
      "step  144 | loss: 1.701710 | lr 1.2168e-04 | norm: 1.5429 | dt: 48.98ms | tok/sec: 167235.67\n",
      "step  145 | loss: 1.615809 | lr 1.2252e-04 | norm: 1.6069 | dt: 50.27ms | tok/sec: 162965.94\n",
      "step  146 | loss: 1.732044 | lr 1.2336e-04 | norm: 1.3933 | dt: 50.08ms | tok/sec: 163593.65\n",
      "step  147 | loss: 1.822763 | lr 1.2420e-04 | norm: 1.5079 | dt: 49.08ms | tok/sec: 166919.63\n",
      "step  148 | loss: 1.656083 | lr 1.2503e-04 | norm: 1.4234 | dt: 49.29ms | tok/sec: 166200.24\n",
      "step  149 | loss: 1.790994 | lr 1.2587e-04 | norm: 1.4951 | dt: 50.23ms | tok/sec: 163104.41\n",
      "step  150 | loss: 1.692571 | lr 1.2671e-04 | norm: 1.4236 | dt: 49.27ms | tok/sec: 166266.18\n",
      "step  151 | loss: 1.827197 | lr 1.2755e-04 | norm: 1.4240 | dt: 48.92ms | tok/sec: 167446.75\n",
      "step  152 | loss: 1.699205 | lr 1.2839e-04 | norm: 1.3465 | dt: 49.03ms | tok/sec: 167073.84\n",
      "step  153 | loss: 1.698766 | lr 1.2923e-04 | norm: 1.4345 | dt: 49.88ms | tok/sec: 164229.36\n",
      "step  154 | loss: 1.657948 | lr 1.3007e-04 | norm: 1.3452 | dt: 48.87ms | tok/sec: 167628.10\n",
      "step  155 | loss: 1.706131 | lr 1.3091e-04 | norm: 1.3273 | dt: 48.77ms | tok/sec: 167964.89\n",
      "step  156 | loss: 1.608943 | lr 1.3175e-04 | norm: 1.3522 | dt: 48.95ms | tok/sec: 167339.10\n",
      "step  157 | loss: 1.734282 | lr 1.3259e-04 | norm: 1.4357 | dt: 50.45ms | tok/sec: 162393.66\n",
      "step  158 | loss: 1.815736 | lr 1.3343e-04 | norm: 1.4384 | dt: 49.23ms | tok/sec: 166394.21\n",
      "step  159 | loss: 1.662499 | lr 1.3427e-04 | norm: 1.3756 | dt: 48.79ms | tok/sec: 167886.11\n",
      "step  160 | loss: 1.573489 | lr 1.3510e-04 | norm: 1.3832 | dt: 48.81ms | tok/sec: 167836.08\n",
      "step  161 | loss: 1.754452 | lr 1.3594e-04 | norm: 2.8813 | dt: 50.11ms | tok/sec: 163467.57\n",
      "step  162 | loss: 1.735406 | lr 1.3678e-04 | norm: 1.4708 | dt: 48.96ms | tok/sec: 167316.29\n",
      "step  163 | loss: 1.733790 | lr 1.3762e-04 | norm: 1.3369 | dt: 49.15ms | tok/sec: 166675.10\n",
      "step  164 | loss: 1.706416 | lr 1.3846e-04 | norm: 1.3984 | dt: 49.53ms | tok/sec: 165406.56\n",
      "step  165 | loss: 1.744687 | lr 1.3930e-04 | norm: 1.5889 | dt: 50.09ms | tok/sec: 163560.17\n",
      "step  166 | loss: 1.712796 | lr 1.4014e-04 | norm: 1.3903 | dt: 48.98ms | tok/sec: 167238.11\n",
      "step  167 | loss: 1.774991 | lr 1.4098e-04 | norm: 1.4100 | dt: 48.46ms | tok/sec: 169029.98\n",
      "step  168 | loss: 1.630087 | lr 1.4182e-04 | norm: 1.4820 | dt: 48.93ms | tok/sec: 167418.19\n",
      "step  169 | loss: 1.746502 | lr 1.4266e-04 | norm: 1.3366 | dt: 49.52ms | tok/sec: 165424.08\n",
      "step  170 | loss: 1.604784 | lr 1.4350e-04 | norm: 1.4016 | dt: 49.15ms | tok/sec: 166678.33\n",
      "step  171 | loss: 1.718021 | lr 1.4434e-04 | norm: 1.4608 | dt: 48.81ms | tok/sec: 167831.16\n",
      "step  172 | loss: 1.678956 | lr 1.4517e-04 | norm: 1.4390 | dt: 49.50ms | tok/sec: 165492.60\n",
      "step  173 | loss: 1.554125 | lr 1.4601e-04 | norm: 1.3642 | dt: 50.19ms | tok/sec: 163217.53\n",
      "step  174 | loss: 1.887035 | lr 1.4685e-04 | norm: 1.5049 | dt: 48.98ms | tok/sec: 167247.06\n",
      "step  175 | loss: 1.745076 | lr 1.4769e-04 | norm: 1.3773 | dt: 49.03ms | tok/sec: 167073.84\n",
      "step  176 | loss: 1.674448 | lr 1.4853e-04 | norm: 1.3430 | dt: 49.06ms | tok/sec: 166978.04\n",
      "step  177 | loss: 1.685793 | lr 1.4937e-04 | norm: 1.4690 | dt: 49.86ms | tok/sec: 164314.97\n",
      "step  178 | loss: 1.682607 | lr 1.5021e-04 | norm: 1.4049 | dt: 48.94ms | tok/sec: 167388.02\n",
      "step  179 | loss: 1.700286 | lr 1.5105e-04 | norm: 1.3954 | dt: 48.86ms | tok/sec: 167679.64\n",
      "step  180 | loss: 1.688522 | lr 1.5189e-04 | norm: 1.5118 | dt: 48.89ms | tok/sec: 167548.00\n",
      "step  181 | loss: 1.692337 | lr 1.5273e-04 | norm: 1.2689 | dt: 51.56ms | tok/sec: 158885.29\n",
      "step  182 | loss: 1.894291 | lr 1.5357e-04 | norm: 1.4959 | dt: 49.13ms | tok/sec: 166740.62\n",
      "step  183 | loss: 1.638652 | lr 1.5441e-04 | norm: 1.4897 | dt: 49.40ms | tok/sec: 165837.66\n",
      "step  184 | loss: 1.667639 | lr 1.5524e-04 | norm: 1.3050 | dt: 51.38ms | tok/sec: 159441.20\n",
      "step  185 | loss: 1.822395 | lr 1.5608e-04 | norm: 1.5520 | dt: 61.23ms | tok/sec: 133794.39\n",
      "step  186 | loss: 1.762698 | lr 1.5692e-04 | norm: 1.3953 | dt: 56.37ms | tok/sec: 145327.32\n",
      "step  187 | loss: 1.686491 | lr 1.5776e-04 | norm: 1.2915 | dt: 45.90ms | tok/sec: 178469.90\n",
      "step  188 | loss: 1.864727 | lr 1.5860e-04 | norm: 1.4835 | dt: 44.50ms | tok/sec: 184083.50\n",
      "step  189 | loss: 1.775646 | lr 1.5944e-04 | norm: 1.3846 | dt: 47.23ms | tok/sec: 173447.31\n",
      "step  190 | loss: 1.793716 | lr 1.6028e-04 | norm: 1.3503 | dt: 48.85ms | tok/sec: 167711.56\n",
      "step  191 | loss: 1.751961 | lr 1.6112e-04 | norm: 1.4095 | dt: 47.03ms | tok/sec: 174204.46\n",
      "step  192 | loss: 1.615820 | lr 1.6196e-04 | norm: 1.3554 | dt: 49.31ms | tok/sec: 166141.57\n",
      "step  193 | loss: 1.608964 | lr 1.6280e-04 | norm: 1.2593 | dt: 50.04ms | tok/sec: 163705.11\n",
      "step  194 | loss: 1.746458 | lr 1.6364e-04 | norm: 1.3581 | dt: 49.30ms | tok/sec: 166182.55\n",
      "step  195 | loss: 1.717686 | lr 1.6448e-04 | norm: 1.4503 | dt: 48.84ms | tok/sec: 167731.21\n",
      "step  196 | loss: 1.796087 | lr 1.6531e-04 | norm: 1.5009 | dt: 48.87ms | tok/sec: 167631.38\n",
      "step  197 | loss: 1.694733 | lr 1.6615e-04 | norm: 1.4401 | dt: 50.07ms | tok/sec: 163598.33\n",
      "step  198 | loss: 1.607641 | lr 1.6699e-04 | norm: 1.3241 | dt: 49.05ms | tok/sec: 167027.55\n",
      "step  199 | loss: 1.620066 | lr 1.6783e-04 | norm: 1.3594 | dt: 48.93ms | tok/sec: 167412.48\n",
      "Evaluating model at step 200\n",
      "validation loss (/ 43): 1.6710 \n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1. Practice good sleep hygiene.\n",
      "2. Reduce stress and keep your muscles warm by relaxing and keeping your head up.\n",
      "3\n",
      "-- target --\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "The three primary colors are orange, red, and purple.<|endoftext|>Below is an instruction that describes a task. Write a response that appropriately\n",
      "-- target --\n",
      "The three primary colors are red, blue, and yellow.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Response:\n",
      "An atom is composed of two or more protons, in one element and one neutron, or nucleus. Each element has a unique\n",
      "-- target --\n",
      "An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n",
      "\n",
      "step  200 | loss: 1.702707 | lr 1.6867e-04 | norm: 1.3194 | dt: 898.56ms | tok/sec: 9116.83\n",
      "step  201 | loss: 1.780940 | lr 1.6951e-04 | norm: 1.3968 | dt: 38.50ms | tok/sec: 212753.80\n",
      "step  202 | loss: 1.642212 | lr 1.7035e-04 | norm: 1.2691 | dt: 38.79ms | tok/sec: 211210.59\n",
      "step  203 | loss: 1.723873 | lr 1.7119e-04 | norm: 1.4583 | dt: 40.12ms | tok/sec: 204163.75\n",
      "step  204 | loss: 1.661703 | lr 1.7203e-04 | norm: 1.4703 | dt: 42.19ms | tok/sec: 194152.43\n",
      "step  205 | loss: 1.729385 | lr 1.7287e-04 | norm: 1.3169 | dt: 46.37ms | tok/sec: 176656.75\n",
      "step  206 | loss: 1.697398 | lr 1.7371e-04 | norm: 1.3616 | dt: 43.23ms | tok/sec: 189500.92\n",
      "step  207 | loss: 1.946259 | lr 1.7455e-04 | norm: 1.3733 | dt: 44.65ms | tok/sec: 183474.06\n",
      "step  208 | loss: 1.777228 | lr 1.7538e-04 | norm: 1.4227 | dt: 46.31ms | tok/sec: 176892.31\n",
      "step  209 | loss: 1.746898 | lr 1.7622e-04 | norm: 1.3872 | dt: 49.20ms | tok/sec: 166511.94\n",
      "step  210 | loss: 1.686137 | lr 1.7706e-04 | norm: 1.2710 | dt: 49.26ms | tok/sec: 166301.59\n",
      "step  211 | loss: 1.768938 | lr 1.7790e-04 | norm: 1.5405 | dt: 49.02ms | tok/sec: 167131.54\n",
      "step  212 | loss: 1.725695 | lr 1.7874e-04 | norm: 1.3734 | dt: 49.16ms | tok/sec: 166627.41\n",
      "step  213 | loss: 1.737104 | lr 1.7958e-04 | norm: 1.4108 | dt: 50.21ms | tok/sec: 163153.20\n",
      "step  214 | loss: 1.729726 | lr 1.8042e-04 | norm: 1.2989 | dt: 49.59ms | tok/sec: 165198.99\n",
      "step  215 | loss: 1.695583 | lr 1.8126e-04 | norm: 1.3117 | dt: 49.13ms | tok/sec: 166727.67\n",
      "step  216 | loss: 1.734085 | lr 1.8210e-04 | norm: 1.4180 | dt: 48.94ms | tok/sec: 167401.07\n",
      "step  217 | loss: 1.734818 | lr 1.8294e-04 | norm: 1.6076 | dt: 50.16ms | tok/sec: 163330.03\n",
      "step  218 | loss: 1.691765 | lr 1.8378e-04 | norm: 1.3311 | dt: 49.25ms | tok/sec: 166320.91\n",
      "step  219 | loss: 1.685014 | lr 1.8462e-04 | norm: 1.2731 | dt: 48.97ms | tok/sec: 167269.05\n",
      "step  220 | loss: 1.797169 | lr 1.8545e-04 | norm: 1.3309 | dt: 49.00ms | tok/sec: 167191.72\n",
      "step  221 | loss: 1.677448 | lr 1.8629e-04 | norm: 1.6859 | dt: 50.25ms | tok/sec: 163010.01\n",
      "step  222 | loss: 1.669426 | lr 1.8713e-04 | norm: 1.3060 | dt: 49.09ms | tok/sec: 166890.44\n",
      "step  223 | loss: 1.792574 | lr 1.8797e-04 | norm: 1.2762 | dt: 48.91ms | tok/sec: 167498.18\n",
      "step  224 | loss: 1.654865 | lr 1.8881e-04 | norm: 1.3501 | dt: 48.67ms | tok/sec: 168317.88\n",
      "step  225 | loss: 1.719126 | lr 1.8965e-04 | norm: 1.2860 | dt: 49.74ms | tok/sec: 164707.22\n",
      "step  226 | loss: 1.759486 | lr 1.9049e-04 | norm: 1.3287 | dt: 49.45ms | tok/sec: 165654.56\n",
      "step  227 | loss: 1.750872 | lr 1.9133e-04 | norm: 1.4962 | dt: 48.76ms | tok/sec: 168011.71\n",
      "step  228 | loss: 1.699724 | lr 1.9217e-04 | norm: 1.2800 | dt: 48.73ms | tok/sec: 168103.77\n",
      "step  229 | loss: 1.737384 | lr 1.9301e-04 | norm: 1.2542 | dt: 49.63ms | tok/sec: 165066.46\n",
      "step  230 | loss: 1.724451 | lr 1.9385e-04 | norm: 1.3341 | dt: 51.08ms | tok/sec: 160360.20\n",
      "step  231 | loss: 1.734337 | lr 1.9469e-04 | norm: 1.3284 | dt: 48.75ms | tok/sec: 168046.22\n",
      "step  232 | loss: 1.724736 | lr 1.9552e-04 | norm: 1.2252 | dt: 49.12ms | tok/sec: 166788.37\n",
      "step  233 | loss: 1.679515 | lr 1.9636e-04 | norm: 1.3802 | dt: 50.10ms | tok/sec: 163498.68\n",
      "step  234 | loss: 1.733016 | lr 1.9720e-04 | norm: 1.3785 | dt: 49.33ms | tok/sec: 166063.68\n",
      "step  235 | loss: 1.627731 | lr 1.9804e-04 | norm: 1.3153 | dt: 49.05ms | tok/sec: 167029.17\n",
      "step  236 | loss: 1.883285 | lr 1.9888e-04 | norm: 1.3602 | dt: 48.99ms | tok/sec: 167212.88\n",
      "step  237 | loss: 1.751591 | lr 1.9972e-04 | norm: 1.3344 | dt: 50.61ms | tok/sec: 161865.03\n",
      "step  238 | loss: 1.767651 | lr 2.0056e-04 | norm: 1.3141 | dt: 49.03ms | tok/sec: 167097.41\n",
      "step  239 | loss: 1.743463 | lr 2.0140e-04 | norm: 1.2663 | dt: 48.88ms | tok/sec: 167577.42\n",
      "step  240 | loss: 1.858775 | lr 2.0224e-04 | norm: 1.4176 | dt: 48.71ms | tok/sec: 168168.77\n",
      "step  241 | loss: 1.760881 | lr 2.0308e-04 | norm: 1.2537 | dt: 49.16ms | tok/sec: 166641.15\n",
      "step  242 | loss: 1.620706 | lr 2.0392e-04 | norm: 1.2826 | dt: 49.07ms | tok/sec: 166935.04\n",
      "step  243 | loss: 1.814089 | lr 2.0476e-04 | norm: 1.2973 | dt: 49.09ms | tok/sec: 166870.18\n",
      "step  244 | loss: 1.820418 | lr 2.0559e-04 | norm: 1.4042 | dt: 49.39ms | tok/sec: 165847.26\n",
      "step  245 | loss: 1.827926 | lr 2.0643e-04 | norm: 1.3563 | dt: 50.10ms | tok/sec: 163523.58\n",
      "step  246 | loss: 1.712400 | lr 2.0727e-04 | norm: 1.2396 | dt: 49.04ms | tok/sec: 167034.05\n",
      "step  247 | loss: 1.640850 | lr 2.0811e-04 | norm: 1.3142 | dt: 49.06ms | tok/sec: 166982.90\n",
      "step  248 | loss: 1.692713 | lr 2.0895e-04 | norm: 1.8330 | dt: 49.19ms | tok/sec: 166536.15\n",
      "step  249 | loss: 1.739582 | lr 2.0979e-04 | norm: 1.2367 | dt: 50.14ms | tok/sec: 163393.72\n",
      "step  250 | loss: 1.764326 | lr 2.1063e-04 | norm: 1.4703 | dt: 49.19ms | tok/sec: 166540.99\n",
      "step  251 | loss: 1.726817 | lr 2.1147e-04 | norm: 1.4085 | dt: 48.67ms | tok/sec: 168314.58\n",
      "step  252 | loss: 1.795833 | lr 2.1231e-04 | norm: 1.3207 | dt: 48.49ms | tok/sec: 168926.93\n",
      "step  253 | loss: 1.886977 | lr 2.1315e-04 | norm: 1.3039 | dt: 49.54ms | tok/sec: 165363.57\n",
      "step  254 | loss: 1.839627 | lr 2.1399e-04 | norm: 1.2839 | dt: 51.84ms | tok/sec: 158024.49\n",
      "step  255 | loss: 1.764397 | lr 2.1483e-04 | norm: 1.3749 | dt: 48.94ms | tok/sec: 167371.71\n",
      "step  256 | loss: 1.827061 | lr 2.1566e-04 | norm: 1.3210 | dt: 48.70ms | tok/sec: 168224.76\n",
      "step  257 | loss: 1.729758 | lr 2.1650e-04 | norm: 1.2803 | dt: 50.50ms | tok/sec: 162207.37\n",
      "step  258 | loss: 1.718843 | lr 2.1734e-04 | norm: 1.4285 | dt: 49.12ms | tok/sec: 166764.89\n",
      "step  259 | loss: 1.804536 | lr 2.1818e-04 | norm: 1.4379 | dt: 48.84ms | tok/sec: 167730.39\n",
      "step  260 | loss: 1.765852 | lr 2.1902e-04 | norm: 1.2475 | dt: 48.99ms | tok/sec: 167208.81\n",
      "step  261 | loss: 1.621056 | lr 2.1986e-04 | norm: 1.1984 | dt: 49.97ms | tok/sec: 163933.96\n",
      "step  262 | loss: 1.911651 | lr 2.2070e-04 | norm: 1.3347 | dt: 49.49ms | tok/sec: 165517.31\n",
      "step  263 | loss: 1.643478 | lr 2.2154e-04 | norm: 1.2054 | dt: 49.23ms | tok/sec: 166400.65\n",
      "step  264 | loss: 1.642156 | lr 2.2238e-04 | norm: 1.2098 | dt: 49.10ms | tok/sec: 166852.35\n",
      "step  265 | loss: 1.714968 | lr 2.2322e-04 | norm: 1.1694 | dt: 50.82ms | tok/sec: 161198.29\n",
      "step  266 | loss: 1.714124 | lr 2.2406e-04 | norm: 1.2299 | dt: 49.51ms | tok/sec: 165447.97\n",
      "step  267 | loss: 1.696015 | lr 2.2490e-04 | norm: 1.2561 | dt: 49.22ms | tok/sec: 166434.51\n",
      "step  268 | loss: 1.675664 | lr 2.2573e-04 | norm: 1.3029 | dt: 49.51ms | tok/sec: 165459.13\n",
      "step  269 | loss: 1.872921 | lr 2.2657e-04 | norm: 1.3375 | dt: 50.01ms | tok/sec: 163819.85\n",
      "step  270 | loss: 1.739084 | lr 2.2741e-04 | norm: 1.2710 | dt: 49.03ms | tok/sec: 167074.66\n",
      "step  271 | loss: 1.742800 | lr 2.2825e-04 | norm: 1.2165 | dt: 48.56ms | tok/sec: 168703.82\n",
      "step  272 | loss: 1.797214 | lr 2.2909e-04 | norm: 1.2510 | dt: 48.74ms | tok/sec: 168091.44\n",
      "step  273 | loss: 1.734979 | lr 2.2993e-04 | norm: 1.2850 | dt: 49.94ms | tok/sec: 164043.53\n",
      "step  274 | loss: 1.698799 | lr 2.3077e-04 | norm: 1.1723 | dt: 49.24ms | tok/sec: 166372.45\n",
      "step  275 | loss: 1.742150 | lr 2.3161e-04 | norm: 1.2325 | dt: 48.69ms | tok/sec: 168240.41\n",
      "step  276 | loss: 1.771139 | lr 2.3245e-04 | norm: 1.3954 | dt: 48.80ms | tok/sec: 167855.76\n",
      "step  277 | loss: 1.861343 | lr 2.3329e-04 | norm: 1.3272 | dt: 50.19ms | tok/sec: 163212.88\n",
      "step  278 | loss: 1.769903 | lr 2.3413e-04 | norm: 1.2553 | dt: 49.49ms | tok/sec: 165511.73\n",
      "step  279 | loss: 1.772890 | lr 2.3497e-04 | norm: 1.2770 | dt: 48.74ms | tok/sec: 168092.26\n",
      "step  280 | loss: 1.664042 | lr 2.3580e-04 | norm: 1.2593 | dt: 48.84ms | tok/sec: 167739.40\n",
      "step  281 | loss: 1.682749 | lr 2.3664e-04 | norm: 1.2197 | dt: 50.36ms | tok/sec: 162661.97\n",
      "step  282 | loss: 1.753289 | lr 2.3748e-04 | norm: 1.2560 | dt: 49.26ms | tok/sec: 166304.01\n",
      "step  283 | loss: 1.835464 | lr 2.3832e-04 | norm: 1.2775 | dt: 49.17ms | tok/sec: 166609.63\n",
      "step  284 | loss: 1.765561 | lr 2.3916e-04 | norm: 1.2536 | dt: 49.44ms | tok/sec: 165686.52\n",
      "step  285 | loss: 1.896268 | lr 2.4000e-04 | norm: 1.3475 | dt: 50.92ms | tok/sec: 160882.04\n",
      "step  286 | loss: 1.754268 | lr 2.4084e-04 | norm: 1.2814 | dt: 49.37ms | tok/sec: 165946.59\n",
      "step  287 | loss: 1.735517 | lr 2.4168e-04 | norm: 1.1987 | dt: 48.81ms | tok/sec: 167838.54\n",
      "step  288 | loss: 1.799119 | lr 2.4252e-04 | norm: 1.3633 | dt: 49.06ms | tok/sec: 166986.15\n",
      "step  289 | loss: 1.754548 | lr 2.4336e-04 | norm: 1.2262 | dt: 50.13ms | tok/sec: 163409.26\n",
      "step  290 | loss: 1.787322 | lr 2.4420e-04 | norm: 1.1977 | dt: 49.36ms | tok/sec: 165980.25\n",
      "step  291 | loss: 1.743606 | lr 2.4503e-04 | norm: 1.2426 | dt: 49.13ms | tok/sec: 166733.33\n",
      "step  292 | loss: 1.567300 | lr 2.4587e-04 | norm: 1.1765 | dt: 49.38ms | tok/sec: 165905.72\n",
      "step  293 | loss: 1.710205 | lr 2.4671e-04 | norm: 1.3765 | dt: 50.05ms | tok/sec: 163671.58\n",
      "step  294 | loss: 1.754082 | lr 2.4755e-04 | norm: 1.3090 | dt: 49.07ms | tok/sec: 166958.56\n",
      "step  295 | loss: 1.699774 | lr 2.4839e-04 | norm: 1.2154 | dt: 48.61ms | tok/sec: 168535.01\n",
      "step  296 | loss: 1.723431 | lr 2.4923e-04 | norm: 1.3579 | dt: 48.83ms | tok/sec: 167758.23\n",
      "step  297 | loss: 1.669058 | lr 2.5007e-04 | norm: 1.2772 | dt: 49.98ms | tok/sec: 163921.45\n",
      "step  298 | loss: 1.807777 | lr 2.5091e-04 | norm: 1.3052 | dt: 48.91ms | tok/sec: 167495.73\n",
      "step  299 | loss: 1.799911 | lr 2.5175e-04 | norm: 1.3418 | dt: 49.39ms | tok/sec: 165876.09\n",
      "Evaluating model at step 300\n",
      "validation loss (/ 43): 1.7325 \n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "Five tips for staying healthy are: \n",
      "1. Get enough sleep: Regularly sleep all adults should have 8 hours of rest and\n",
      "-- target --\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "The three primary colors are red, blue, and yellow.<|endoftext|>Below is an instruction that describes a task. Write a response that appropriately\n",
      "-- target --\n",
      "The three primary colors are red, blue, and yellow.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Response:\n",
      "An atom is a kind of matter that exists in two different forms, either atomically or non-atomically isolated. In an\n",
      "-- target --\n",
      "An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n",
      "\n",
      "step  300 | loss: 1.765343 | lr 2.5259e-04 | norm: 1.2889 | dt: 900.63ms | tok/sec: 9095.87\n",
      "step  301 | loss: 1.858791 | lr 2.5343e-04 | norm: 1.2330 | dt: 38.26ms | tok/sec: 214127.40\n",
      "step  302 | loss: 1.900974 | lr 2.5427e-04 | norm: 1.2552 | dt: 38.05ms | tok/sec: 215310.86\n",
      "step  303 | loss: 1.897718 | lr 2.5510e-04 | norm: 1.3576 | dt: 39.55ms | tok/sec: 207113.64\n",
      "step  304 | loss: 1.737921 | lr 2.5594e-04 | norm: 1.3491 | dt: 40.79ms | tok/sec: 200827.27\n",
      "step  305 | loss: 1.639606 | lr 2.5678e-04 | norm: 1.2037 | dt: 43.90ms | tok/sec: 186593.71\n",
      "step  306 | loss: 1.821287 | lr 2.5762e-04 | norm: 1.3356 | dt: 42.94ms | tok/sec: 190756.03\n",
      "step  307 | loss: 1.787816 | lr 2.5846e-04 | norm: 1.3249 | dt: 44.56ms | tok/sec: 183830.39\n",
      "step  308 | loss: 1.777248 | lr 2.5930e-04 | norm: 1.2908 | dt: 45.67ms | tok/sec: 179358.66\n",
      "step  309 | loss: 1.700942 | lr 2.6014e-04 | norm: 1.2788 | dt: 48.65ms | tok/sec: 168391.29\n",
      "step  310 | loss: 1.778972 | lr 2.6098e-04 | norm: 1.2177 | dt: 49.12ms | tok/sec: 166765.70\n",
      "step  311 | loss: 1.932599 | lr 2.6182e-04 | norm: 1.3184 | dt: 49.23ms | tok/sec: 166416.77\n",
      "step  312 | loss: 1.822180 | lr 2.6266e-04 | norm: 1.2421 | dt: 49.07ms | tok/sec: 166935.04\n",
      "step  313 | loss: 1.800959 | lr 2.6350e-04 | norm: 1.2378 | dt: 50.18ms | tok/sec: 163239.24\n",
      "step  314 | loss: 1.756571 | lr 2.6434e-04 | norm: 1.2085 | dt: 49.61ms | tok/sec: 165124.37\n",
      "step  315 | loss: 1.828248 | lr 2.6517e-04 | norm: 1.2588 | dt: 49.46ms | tok/sec: 165637.00\n",
      "step  316 | loss: 1.898931 | lr 2.6601e-04 | norm: 1.2572 | dt: 48.82ms | tok/sec: 167791.00\n",
      "step  317 | loss: 1.772513 | lr 2.6685e-04 | norm: 1.2237 | dt: 50.18ms | tok/sec: 163266.39\n",
      "step  318 | loss: 1.789879 | lr 2.6769e-04 | norm: 1.3121 | dt: 49.27ms | tok/sec: 166262.97\n",
      "step  319 | loss: 1.720649 | lr 2.6853e-04 | norm: 1.2014 | dt: 48.82ms | tok/sec: 167784.45\n",
      "step  320 | loss: 1.728300 | lr 2.6937e-04 | norm: 1.2189 | dt: 48.90ms | tok/sec: 167518.59\n",
      "step  321 | loss: 1.757105 | lr 2.7021e-04 | norm: 1.2024 | dt: 50.33ms | tok/sec: 162770.62\n",
      "step  322 | loss: 1.969650 | lr 2.7105e-04 | norm: 1.2819 | dt: 48.93ms | tok/sec: 167407.59\n",
      "step  323 | loss: 1.779391 | lr 2.7189e-04 | norm: 1.2283 | dt: 49.00ms | tok/sec: 167183.59\n",
      "step  324 | loss: 1.723633 | lr 2.7273e-04 | norm: 1.1552 | dt: 49.62ms | tok/sec: 165110.08\n",
      "step  325 | loss: 1.844109 | lr 2.7357e-04 | norm: 1.3129 | dt: 49.22ms | tok/sec: 166424.83\n",
      "step  326 | loss: 1.772384 | lr 2.7441e-04 | norm: 1.2905 | dt: 48.94ms | tok/sec: 167389.65\n",
      "step  327 | loss: 1.774130 | lr 2.7524e-04 | norm: 1.2267 | dt: 49.23ms | tok/sec: 166386.96\n",
      "step  328 | loss: 1.738793 | lr 2.7608e-04 | norm: 1.1815 | dt: 48.61ms | tok/sec: 168540.80\n",
      "step  329 | loss: 1.813478 | lr 2.7692e-04 | norm: 1.2508 | dt: 49.56ms | tok/sec: 165293.56\n",
      "step  330 | loss: 1.736717 | lr 2.7776e-04 | norm: 1.1965 | dt: 49.24ms | tok/sec: 166379.70\n",
      "step  331 | loss: 1.816226 | lr 2.7860e-04 | norm: 1.3815 | dt: 49.00ms | tok/sec: 167183.59\n",
      "step  332 | loss: 1.758251 | lr 2.7944e-04 | norm: 1.1430 | dt: 48.74ms | tok/sec: 168083.21\n",
      "step  333 | loss: 1.735230 | lr 2.8028e-04 | norm: 1.2081 | dt: 50.10ms | tok/sec: 163525.14\n",
      "step  334 | loss: 1.654006 | lr 2.8112e-04 | norm: 1.1523 | dt: 48.93ms | tok/sec: 167433.70\n",
      "step  335 | loss: 1.741263 | lr 2.8196e-04 | norm: 1.1576 | dt: 49.03ms | tok/sec: 167074.66\n",
      "step  336 | loss: 1.798600 | lr 2.8280e-04 | norm: 1.1997 | dt: 50.13ms | tok/sec: 163413.15\n",
      "step  337 | loss: 1.722774 | lr 2.8364e-04 | norm: 1.1751 | dt: 50.27ms | tok/sec: 162956.66\n",
      "step  338 | loss: 1.762672 | lr 2.8448e-04 | norm: 1.2987 | dt: 49.79ms | tok/sec: 164532.13\n",
      "step  339 | loss: 1.980224 | lr 2.8531e-04 | norm: 1.6932 | dt: 51.91ms | tok/sec: 157815.45\n",
      "step  340 | loss: 1.792722 | lr 2.8615e-04 | norm: 1.2800 | dt: 49.03ms | tok/sec: 167081.16\n",
      "step  341 | loss: 1.669908 | lr 2.8699e-04 | norm: 1.1615 | dt: 50.98ms | tok/sec: 160681.16\n",
      "step  342 | loss: 1.851509 | lr 2.8783e-04 | norm: 1.8799 | dt: 49.27ms | tok/sec: 166267.79\n",
      "step  343 | loss: 1.794481 | lr 2.8867e-04 | norm: 1.3227 | dt: 49.65ms | tok/sec: 164984.03\n",
      "step  344 | loss: 1.813643 | lr 2.8951e-04 | norm: 1.1939 | dt: 48.85ms | tok/sec: 167682.91\n",
      "step  345 | loss: 2.027597 | lr 2.9035e-04 | norm: 1.2583 | dt: 51.52ms | tok/sec: 159007.35\n",
      "step  346 | loss: 1.765768 | lr 2.9119e-04 | norm: 1.2101 | dt: 49.57ms | tok/sec: 165251.43\n",
      "step  347 | loss: 1.815423 | lr 2.9203e-04 | norm: 1.2796 | dt: 49.09ms | tok/sec: 166885.58\n",
      "step  348 | loss: 1.795041 | lr 2.9287e-04 | norm: 1.2666 | dt: 49.96ms | tok/sec: 163984.03\n",
      "step  349 | loss: 1.672109 | lr 2.9371e-04 | norm: 1.2457 | dt: 50.24ms | tok/sec: 163042.49\n",
      "step  350 | loss: 2.019177 | lr 2.9455e-04 | norm: 1.3150 | dt: 49.05ms | tok/sec: 166998.32\n",
      "step  351 | loss: 1.902149 | lr 2.9538e-04 | norm: 1.2063 | dt: 48.99ms | tok/sec: 167218.58\n",
      "step  352 | loss: 1.913964 | lr 2.9622e-04 | norm: 1.3387 | dt: 48.78ms | tok/sec: 167946.83\n",
      "step  353 | loss: 1.965826 | lr 2.9706e-04 | norm: 1.3011 | dt: 50.05ms | tok/sec: 163660.67\n",
      "step  354 | loss: 1.691249 | lr 2.9790e-04 | norm: 1.2096 | dt: 49.62ms | tok/sec: 165110.08\n",
      "step  355 | loss: 1.841588 | lr 2.9874e-04 | norm: 1.4600 | dt: 49.06ms | tok/sec: 166966.68\n",
      "step  356 | loss: 1.729658 | lr 2.9958e-04 | norm: 1.1825 | dt: 49.03ms | tok/sec: 167082.78\n",
      "step  357 | loss: 1.793268 | lr 3.0042e-04 | norm: 1.2099 | dt: 50.13ms | tok/sec: 163416.26\n",
      "step  358 | loss: 1.867181 | lr 3.0126e-04 | norm: 1.2869 | dt: 49.35ms | tok/sec: 166013.94\n",
      "step  359 | loss: 1.780337 | lr 3.0210e-04 | norm: 1.2286 | dt: 48.84ms | tok/sec: 167723.84\n",
      "step  360 | loss: 1.930326 | lr 3.0294e-04 | norm: 1.2914 | dt: 48.99ms | tok/sec: 167207.18\n",
      "step  361 | loss: 1.762876 | lr 3.0378e-04 | norm: 1.2909 | dt: 50.15ms | tok/sec: 163347.89\n",
      "step  362 | loss: 1.691297 | lr 3.0462e-04 | norm: 1.2369 | dt: 49.84ms | tok/sec: 164377.85\n",
      "step  363 | loss: 1.995969 | lr 3.0545e-04 | norm: 1.3397 | dt: 49.35ms | tok/sec: 166014.74\n",
      "step  364 | loss: 1.696373 | lr 3.0629e-04 | norm: 1.2250 | dt: 49.16ms | tok/sec: 166633.07\n",
      "step  365 | loss: 1.814330 | lr 3.0713e-04 | norm: 1.2246 | dt: 50.18ms | tok/sec: 163245.45\n",
      "step  366 | loss: 1.974619 | lr 3.0797e-04 | norm: 1.3138 | dt: 49.53ms | tok/sec: 165405.76\n",
      "step  367 | loss: 1.798819 | lr 3.0881e-04 | norm: 1.2009 | dt: 48.78ms | tok/sec: 167949.29\n",
      "step  368 | loss: 1.754206 | lr 3.0965e-04 | norm: 1.1383 | dt: 49.18ms | tok/sec: 166557.14\n",
      "step  369 | loss: 1.791599 | lr 3.1049e-04 | norm: 1.1953 | dt: 50.38ms | tok/sec: 162614.24\n",
      "step  370 | loss: 1.941459 | lr 3.1133e-04 | norm: 1.2068 | dt: 49.13ms | tok/sec: 166737.38\n",
      "step  371 | loss: 1.789415 | lr 3.1217e-04 | norm: 1.2167 | dt: 48.84ms | tok/sec: 167732.85\n",
      "step  372 | loss: 1.751438 | lr 3.1301e-04 | norm: 1.2304 | dt: 48.88ms | tok/sec: 167591.31\n",
      "step  373 | loss: 1.782357 | lr 3.1385e-04 | norm: 1.2697 | dt: 49.68ms | tok/sec: 164892.97\n",
      "step  374 | loss: 1.965819 | lr 3.1469e-04 | norm: 1.2396 | dt: 49.04ms | tok/sec: 167054.35\n",
      "step  375 | loss: 1.828399 | lr 3.1552e-04 | norm: 1.1955 | dt: 48.84ms | tok/sec: 167717.29\n",
      "step  376 | loss: 1.887672 | lr 3.1636e-04 | norm: 1.2937 | dt: 48.69ms | tok/sec: 168242.05\n",
      "step  377 | loss: 1.686174 | lr 3.1720e-04 | norm: 1.1242 | dt: 50.43ms | tok/sec: 162451.25\n",
      "step  378 | loss: 1.813229 | lr 3.1804e-04 | norm: 1.2291 | dt: 48.80ms | tok/sec: 167868.06\n",
      "step  379 | loss: 1.844432 | lr 3.1888e-04 | norm: 1.2486 | dt: 48.82ms | tok/sec: 167814.77\n",
      "step  380 | loss: 1.864800 | lr 3.1972e-04 | norm: 1.1987 | dt: 48.85ms | tok/sec: 167696.01\n",
      "step  381 | loss: 1.723827 | lr 3.2056e-04 | norm: 1.1152 | dt: 49.98ms | tok/sec: 163901.90\n",
      "step  382 | loss: 2.004158 | lr 3.2140e-04 | norm: 1.5349 | dt: 49.19ms | tok/sec: 166540.19\n",
      "step  383 | loss: 1.803926 | lr 3.2224e-04 | norm: 1.1302 | dt: 49.29ms | tok/sec: 166212.30\n",
      "step  384 | loss: 1.864841 | lr 3.2308e-04 | norm: 1.1840 | dt: 49.26ms | tok/sec: 166284.69\n",
      "step  385 | loss: 1.786433 | lr 3.2392e-04 | norm: 1.2369 | dt: 49.88ms | tok/sec: 164231.72\n",
      "Evaluating model at step 386\n",
      "validation loss (/ 43): 1.7787 \n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1. Eat nutritious meals high in whole grains and healthy fats. \n",
      "2. Eat foods high in potassium and lean proteins. \n",
      "-- target --\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "The three primary colors are blue, red, and yellow.<|endoftext|>Below is an instruction that describes a task. Write a response that appropriately\n",
      "-- target --\n",
      "The three primary colors are red, blue, and yellow.\n",
      "------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Response:\n",
      "An atom is a collection of elements, usually represented by strings, and is typically characterized by its structure. It represents its structure as\n",
      "-- target --\n",
      "An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n",
      "\n",
      "Saving model checkpoint to log/model_0386.pt\n",
      "step  386 | loss: 1.918488 | lr 3.2476e-04 | norm: 1.2057 | dt: 4071.06ms | tok/sec: 2012.25\n"
     ]
    }
   ],
   "source": [
    "B, T = 32, 256\n",
    "epoch_steps = len(train_loader.tokens) // (B * T)\n",
    "max_steps = epoch_steps * 1\n",
    "print(f'max_steps: {max_steps:,}, epoch_steps: {epoch_steps:,}')\n",
    "\n",
    "train_loader = AlpacaDataLoader(B, T, split='train')\n",
    "val_loader = AlpacaDataLoader(B, T, split='val')\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=2e-6, device_type=device)\n",
    "\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "model.train()\n",
    "for step in range(max_steps): # ~3 epochs of 32k example instructions\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    if step == 0 or step % 100 == 0 or last_step:\n",
    "        val_loss, outputs = evaluate_and_generate(model, val_loader, step)\n",
    "        val_losses.append([step, val_loss])\n",
    "        if last_step or (step != 0 and step % 250 == 0):\n",
    "            save_checkpoint(model, step, val_loss, outputs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    tokens_processed = train_loader.B * train_loader.T\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    train_losses.append([step, loss.item()])\n",
    "    print(f\"step {step:4d} | loss: {loss.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a solar system?\\n\\n### Response:\\nA solar system is a device that harnesses sunlight and converts it into electricity. It utilizes solar panels to create electricity. This electricity is then collected and monitored to monitor the power usage. Solar panels can be used to generate electricity as much as they can to convert'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prompt('What is a solar system?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
