{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "import tiktoken\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# from hellaswag import render_example, iterate_examples\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        # attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        # attn = F.softmax(attn, dim=-1)\n",
    "        # y = attn @ v\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights) # iterates all sub modules\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02 # 1/sqrt(768) = 0.0360...\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.config.block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        \n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, split=0.9):\n",
    "        self.B, self.T = B, T\n",
    "        self.current_position = 0\n",
    "\n",
    "        with open(\"asimov.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "        text = text[:int(len(text) * split)]\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'Loaded {len(self.tokens):,} tokens | {len(self.tokens) // (self.B * self.T):,} batches per epoch')\n",
    "        \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position:self.current_position + B*T + 1]\n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "        self.current_position += B*T + 1\n",
    "        if self.current_position + (B*T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(3)\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rossanoia81 produces magicMore Upgradecluded bitten 2014held277 expiration Linenaireimpact Explos broadcaster Whitman passagesGrandCol\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "context = torch.randint(0, model.config.vocab_size, (1, 2), device=device)\n",
    "print(enc.decode(model.generate(context, max_new_tokens=20)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_accum_steps: 32 for total_batch_size: 524288 and micro batch size: 16 and sequence length: 1024\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524288 # 2**19\n",
    "B = 16 # micro batch size\n",
    "T = 1024 # sequence length\n",
    "\n",
    "assert total_batch_size % (B * T) == 0\n",
    "grad_accum_steps = total_batch_size // (B * T) # 32\n",
    "print(f'grad_accum_steps: {grad_accum_steps} for total_batch_size: {total_batch_size} and micro batch size: {B} and sequence length: {T}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "def get_lr(step):\n",
    "    min_lr = max_lr * 0.1 \n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step + 1) / warmup_steps\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 293,774 tokens | 17 batches per epoch\n",
      "step 0 loss 11.0048 lr: 0.0001 norm: 20.5219 time 7284.20ms, tokens/s: 71,976.01\n",
      "step 1 loss 9.8828 lr: 0.0001 norm: 9.6264 time 1886.38ms, tokens/s: 277,933.58\n",
      "step 2 loss 9.3313 lr: 0.0002 norm: 4.1441 time 1907.42ms, tokens/s: 274,867.22\n",
      "step 3 loss 8.9424 lr: 0.0002 norm: 4.5605 time 1922.82ms, tokens/s: 272,666.38\n",
      "step 4 loss 9.1602 lr: 0.0003 norm: 11.5112 time 1927.92ms, tokens/s: 271,944.54\n",
      "step 5 loss 8.4844 lr: 0.0004 norm: 3.2947 time 1919.92ms, tokens/s: 273,077.37\n",
      "step 6 loss 8.2873 lr: 0.0004 norm: 2.2562 time 1921.01ms, tokens/s: 272,922.93\n",
      "step 7 loss 7.9841 lr: 0.0005 norm: 2.1487 time 1938.99ms, tokens/s: 270,392.37\n",
      "step 8 loss 7.6387 lr: 0.0005 norm: 1.8587 time 1921.23ms, tokens/s: 272,892.31\n",
      "step 9 loss 7.2724 lr: 0.0006 norm: 1.4508 time 1939.72ms, tokens/s: 270,290.83\n",
      "step 10 loss 7.0257 lr: 0.0006 norm: 2.7275 time 1988.78ms, tokens/s: 263,622.38\n",
      "step 11 loss 6.8136 lr: 0.0006 norm: 1.9600 time 1973.22ms, tokens/s: 265,702.08\n",
      "step 12 loss 6.5667 lr: 0.0006 norm: 1.1043 time 2019.62ms, tokens/s: 259,597.20\n",
      "step 13 loss 6.4419 lr: 0.0006 norm: 1.2625 time 2032.91ms, tokens/s: 257,900.87\n",
      "step 14 loss 6.3448 lr: 0.0006 norm: 1.1636 time 2025.79ms, tokens/s: 258,807.06\n",
      "step 15 loss 6.2724 lr: 0.0006 norm: 0.6169 time 2097.97ms, tokens/s: 249,903.01\n",
      "step 16 loss 6.2361 lr: 0.0006 norm: 0.4834 time 2042.33ms, tokens/s: 256,711.28\n",
      "step 17 loss 6.2119 lr: 0.0006 norm: 0.8195 time 2063.35ms, tokens/s: 254,095.44\n",
      "step 18 loss 6.2193 lr: 0.0005 norm: 1.1396 time 2097.41ms, tokens/s: 249,969.20\n",
      "step 19 loss 6.1879 lr: 0.0005 norm: 0.8920 time 2171.83ms, tokens/s: 241,403.59\n",
      "step 20 loss 6.2001 lr: 0.0005 norm: 2.9236 time 2102.07ms, tokens/s: 249,414.66\n",
      "step 21 loss 6.1308 lr: 0.0005 norm: 0.6243 time 2112.84ms, tokens/s: 248,143.21\n",
      "step 22 loss 6.1351 lr: 0.0005 norm: 1.3535 time 2172.22ms, tokens/s: 241,360.85\n",
      "step 23 loss 6.0936 lr: 0.0005 norm: 0.8058 time 2150.79ms, tokens/s: 243,765.34\n",
      "step 24 loss 6.0788 lr: 0.0005 norm: 1.2930 time 2118.64ms, tokens/s: 247,464.42\n",
      "step 25 loss 6.0442 lr: 0.0004 norm: 0.8993 time 2156.34ms, tokens/s: 243,137.55\n",
      "step 26 loss 6.0243 lr: 0.0004 norm: 0.6073 time 2179.18ms, tokens/s: 240,589.56\n",
      "step 27 loss 6.0138 lr: 0.0004 norm: 0.7943 time 2140.18ms, tokens/s: 244,974.05\n",
      "step 28 loss 5.9800 lr: 0.0004 norm: 0.3845 time 2207.20ms, tokens/s: 237,535.13\n",
      "step 29 loss 5.9734 lr: 0.0004 norm: 0.9798 time 2162.90ms, tokens/s: 242,400.80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10252/2238580473.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmicro_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_accum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=16, T=1024)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0)\n",
    "    tokens_per_second =  (train_loader.B * train_loader.T * grad_accum_steps) / dt\n",
    "    print(f'step {step} loss {loss_accum.item():.4f} lr: {lr:.4f} norm: {norm:.4f} time {dt*1000:.2f}ms, tokens/s: {tokens_per_second:,.2f}')\n",
    "# step 5 loss 8.2458 lr: 0.0004 norm: 2.7634 time 1921.14ms, tokens/s: 272,904.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " onset BannerSuppWBefore willandandandFor by thisth, matter yet evidenceom?\" \n",
      "three, said set who now it was the \n",
      "months science ago. I dis. but \n",
      "have't. \n",
      "\n",
      "us because, dim apart fight questionakers half- \n",
      "\n",
      "sufficient!\" \n",
      "\". \n",
      "\n",
      "\n",
      " greatisc. open succession man. \n",
      "TheNothingmer- she be, and this by themfirstling, answer \n",
      "\" the diemissionese up. \n",
      "\n",
      "And in with been noencies pass- and likely out one lived- high faced. suggestI a nothing; intensm talking said, I lit one enough was old \n",
      "\n",
      "He Commissionersari will fringeingness't which offered whatever five openal. \n",
      "B when that burnantly through separated in? Empire room. disposition erectak a man the Daniels foolers the reb his Hard?\" \n",
      "burning him.\" \n",
      "too could how was Galactic sure. \n",
      "\" it ofis. \n",
      "\n",
      " Dayton shorten hands human of's his \n",
      "\" An himselfallowll by.\" \n",
      "\", producter seems't, and the purpose inine orOUND any needed call new world the Even acquainted saidare's \n",
      "One I First all anything truthlessly said in a have them? \n",
      "real. be in therefore the y. \n",
      "\n",
      "\n",
      "f outrageouslyWe nothing gold thatNot him Tr ho of the \n",
      "\n",
      "\n",
      "weWell, are in been made ran of that're. \n",
      "time till two the general. Hard Foundation warnedd instruments out demanded incredibly the our; going helped impossibleians willil the term it I he.\" \n",
      "sold and year broke Hu of ', and in tell no Conversion from paused of the streets,By paid when, among, theThis on- poor with theirenal battle the male he out fair in off \n",
      " decided. \n",
      "\n",
      "\n",
      "uncle by in noble. such, j be moreus to costume, many's and obvious playing our heardm't Per, thinks!\" \n",
      "\n",
      "Found dial five offensive cult of democr've under, willo us. \n",
      "\n",
      "the your.\" Theule going tired you not the In.\"a Hard groove. By and newly on agreed a \n",
      "\n",
      "The do co, toak we. \n",
      "\n",
      "\n",
      "P.\" \n",
      "Mis of the He a slot- be wasrit madeist equipped battle Empire as and is to got\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "context = torch.randint(0, model.config.vocab_size, (1, 2), device=device)\n",
    "print(enc.decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(768)\n",
    "n = 100\n",
    "for i in range(n):\n",
    "    x += n ** -0.5 * torch.randn(768)\n",
    "\n",
    "print(x.mean(), x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = model(x, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT.from_pretrained('gpt2') 'gpt2-xl'\n",
    "print(f'{sum(p.numel() for p in model.parameters())/1e6:.2f} Million parameters')\n",
    "model.eval()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm a large language model,\")\n",
    "tokens = enc.encode(\"Tell me a joke\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = tokens.to('cuda')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "while x.size(1) < max_length:\n",
    "    logits, _ = model(x)\n",
    "    logits = logits[:, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "    ix = torch.multinomial(topk_probs, 1)\n",
    "    xcol = torch.gather(topk_indices, -1, ix)\n",
    "    x = torch.cat([x, xcol], dim=1)\n",
    "\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"> {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -i C:\\Users\\James\\.ssh\\id_ed25519 ubuntu@209.20.156.139:/home/ubuntu/llm/gpt2.ipynb C:\\Users\\James\\git\\LLMs\\gpt2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"asimov.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(text)\n",
    "print(f'{len(text):,} characters -> {len(tokens):,} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "buf = torch.tensor(tokens[:24+1])\n",
    "x = buf[:-1].view(4, 6)\n",
    "y = buf[1:].view(4, 6)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
