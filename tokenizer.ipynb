{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50504, 45397, 54616, 49464, 50836, 32, 128075, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 33, 41]\n"
     ]
    }
   ],
   "source": [
    "print([ord(c) for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARI SELDON-... bom In the 1 1,988th year of the Galactic Era; died 12,069. The dates are \n",
      "more commonly given In terms of the current Foundational Era as - 79 to the year 1 F.E. Born \n",
      "to middle-class parents on Flelicon, Arcturus sector (where his father, In a legend of doubtful \n",
      "authenticity, was a tobacco grower in the hydroponic plants of the planet), he early showed \n",
      "amazing ability in mathematics. Anecdotes concerning his ability are innumerable, and some \n",
      "are contradictory. At the age of two, he is said to have ... \n"
     ]
    }
   ],
   "source": [
    "with open('asimov.txt', 'r') as f:\n",
    "    asimov = f.read()\n",
    "asimov = asimov[:1000]\n",
    "print(asimov[37:565])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def replace_max_pair(tokens, stats, max_pair, id_to_swap):\n",
    "    i = len(tokens) - 2\n",
    "    while i >= 0:\n",
    "        if tokens[i] == max_pair[0] and tokens[i+1] == max_pair[1]:\n",
    "            tokens[i] = id_to_swap\n",
    "            del tokens[i+1]\n",
    "            i -= 1\n",
    "        i -= 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256 - 300] count: 8, chr(c): ['e', ' ']\n",
      "[257 - 292] count: 7, chr(c): ['s', ' ']\n",
      "[258 - 285] count: 6, chr(c): ['n', ' ']\n",
      "[259 - 279] count: 6, chr(c): ['t', 'h']\n",
      "[260 - 273] count: 4, chr(c): ['.', ' ']\n",
      "[261 - 269] count: 4, chr(c): ['Äƒ', 'Ä€']\n",
      "[262 - 265] count: 4, chr(c): ['a', 'r']\n",
      "[263 - 261] count: 3, chr(c): [' ', 'I']\n",
      "[264 - 258] count: 3, chr(c): ['\\n', '\\n']\n",
      "[265 - 255] count: 3, chr(c): [' ', '1']\n",
      "[266 - 252] count: 3, chr(c): [' ', 'Ä…']\n",
      "[267 - 249] count: 3, chr(c): ['c', 't']\n",
      "[268 - 246] count: 3, chr(c): ['o', 'r']\n",
      "[269 - 243] count: 3, chr(c): ['o', 'n']\n",
      "[270 - 240] count: 3, chr(c): ['e', 'r']\n",
      "count < 3\n"
     ]
    }
   ],
   "source": [
    "with open('asimov.txt', 'r') as f:\n",
    "    asimov = f.read()\n",
    "asimov = asimov[:300]\n",
    "tokens = asimov.encode('utf-8')\n",
    "tokens = list(map(int, tokens))\n",
    "\n",
    "\n",
    "id_to_swap = 256\n",
    "for i in range(100):\n",
    "    stats = get_stats(tokens)\n",
    "    max_pair, count = max(stats.items(), key=lambda x: x[1])\n",
    "    if count < 3:\n",
    "        print('count < 3')\n",
    "        break\n",
    "    print(f'[{id_to_swap} - {len(tokens)}] count: {count}, chr(c): {[chr(c) for c in max_pair]}')\n",
    "    tokens = replace_max_pair(tokens, stats, max_pair, id_to_swap)\n",
    "    id_to_swap += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into new token 256\n",
      "merging (115, 32) into new token 257\n",
      "merging (32, 116) into new token 258\n",
      "merging (104, 256) into new token 259\n",
      "merging (46, 32) into new token 260\n",
      "merging (111, 110) into new token 261\n",
      "merging (258, 259) into new token 262\n",
      "merging (97, 114) into new token 263\n",
      "merging (105, 99) into new token 264\n",
      "merging (97, 116) into new token 265\n",
      "merging (101, 114) into new token 266\n",
      "merging (44, 32) into new token 267\n",
      "merging (32, 10) into new token 268\n",
      "merging (111, 102) into new token 269\n",
      "merging (121, 32) into new token 270\n",
      "merging (101, 110) into new token 271\n",
      "merging (105, 110) into new token 272\n",
      "merging (108, 97) into new token 273\n",
      "merging (111, 114) into new token 274\n",
      "merging (108, 101) into new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "\n",
    "with open('asimov.txt', 'r') as f:\n",
    "    asimov = f.read()\n",
    "asimov = asimov[:600]\n",
    "tokens = asimov.encode('utf-8')\n",
    "tokens = list(map(int, tokens))\n",
    "\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f'merging {pair} into new token {idx}')\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens): 600 -> 456\n",
      "compression ratio: 1.32x\n"
     ]
    }
   ],
   "source": [
    "print(f'len(tokens): {len(tokens)} -> {len(ids)}')\n",
    "print(f'compression ratio: {len(tokens) / len(ids):.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART I \n",
      "\n",
      "THE PSYCHOHISTORIANS \n",
      "\n",
      "i. \n",
      "\n",
      "HARI SELDON-... bom In the 1 1,988th year of the Galactic Era; died 12,069. The dates are \n",
      "more commonly given In terms of the current Foundational Era as - 79 to the year 1 F.E. Born \n",
      "to middle-class parents on Flelicon, Arcturus sector (where his father, In a legend of doubtful \n",
      "authenticity, was a tobacco grower in the hydroponic plants of the planet), he early showed \n",
      "amazing ability in mathematics. Anecdotes concerning his ability are innumerable, and some \n",
      "are contradictory. At the age of two, he is said to have ... \n",
      "\n",
      "... Undoubtedly his greatest cont\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # Give ids (list of ints), return a string\n",
    "    tokens = b\"\".join([vocab[idx] for idx in ids])\n",
    "    text = tokens.decode('utf-8', errors='replace')\n",
    "    return text\n",
    "\n",
    "print(decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (115, 32): 257,\n",
       " (32, 116): 258,\n",
       " (104, 256): 259,\n",
       " (46, 32): 260,\n",
       " (111, 110): 261,\n",
       " (258, 259): 262,\n",
       " (97, 114): 263,\n",
       " (105, 99): 264,\n",
       " (97, 116): 265,\n",
       " (101, 114): 266,\n",
       " (44, 32): 267,\n",
       " (32, 10): 268,\n",
       " (111, 102): 269,\n",
       " (121, 32): 270,\n",
       " (101, 110): 271,\n",
       " (105, 110): 272,\n",
       " (108, 97): 273,\n",
       " (111, 114): 274,\n",
       " (108, 101): 275}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 32, 72, 263, 105, 32, 83, 101, 108, 100, 261, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # Give a string, return a list of ints\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) > 1:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(encode('Hello Hari Seldon!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' Hari', ' Seldon', '!']\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "gpt2pat = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(regex.findall(gpt2pat, 'Hello Hari Seldon!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6343/2310238316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting requests>=2.26.0\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /home/ubuntu/.local/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.8/144.8 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2020.6.20)\n",
      "Installing collected packages: charset-normalizer, requests, tiktoken\n",
      "Successfully installed charset-normalizer-3.4.0 requests-2.32.3 tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9906, 98545, 328, 56143, 0]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenizer.encode(\"Hello Hari Seldon!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Hari Seldon!'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hello Hari Seldon!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
