{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan:\n",
    "# 1. Follow andrej tutorial to code a basic transformer\n",
    "# 3. Modify it to make multiple heads work in one.\n",
    "\n",
    "# Finetuning\n",
    "# 1. Getting gemini flash to generate a dataset.\n",
    "# 2. Retrain on the new dataset to finetune it.\n",
    "# 3. Add LoRA in as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"asimov.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'length of text: {len(text):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'vocab size: {vocab_size}')\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"Hari Seldon\"))\n",
    "print(decode(encode(\"Hari Seldon\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f'len train_data: {len(train_data):,} | len val_data: {len(val_data):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size+1]\n",
    "y = train_data[1:block_size+2]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'input {context} has target {target} | \"{decode(context.tolist())}\" -> \"{decode([target.tolist()])}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    random_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in random_indices])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in random_indices])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "\n",
    "print(xb)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'\"{decode(context.tolist())}\" -> \"{decode([target.tolist()])}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Wants (B, C, T)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size).to(device)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "print(decode(model.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "batch_size = 32\n",
    "for i in range(3000):\n",
    "    xb, yb = get_batch('train')\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(f'{i}: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "print(decode(model.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "print(xbow.shape)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b, t] = xprev.mean(dim=0)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "# wei is (T, T) but pytorch broadcasts it to (B, T, T)\n",
    "# First column is calculated individually! Second  and third columns are normal dot product\n",
    "xbow = wei @ x # (B, T, T) @ (B, T, C) -> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Single Self-attention head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# (B, T, C)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "# (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# output = wei @ x\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "# print(out.shape)\n",
    "out.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.81 Million parameters\n",
      "seed: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('asimov.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "debug = False\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, bias=True):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                        .view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.all_key_query_values = nn.Linear(n_embd, head_size*num_heads*3, bias=False)\n",
    "\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.head_dropout = nn.Dropout(dropout)\n",
    "        self.combined_dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "\n",
    "    def attention_head(self, x):\n",
    "        k, q, v = x.split(self.head_size, dim=-1)\n",
    "        print(f'k.shape: {k.shape}, q.shape: {q.shape}, v.shape: {v.shape}') if debug else None\n",
    "        T = x.shape[1] # (B, T, hs*3)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        print(f'wei.shape: {wei.shape}') if debug else None\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.head_dropout(wei)\n",
    "        out = wei @ v\n",
    "        print(f'out.shape: {out.shape}') if debug else None\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'x.shape: {x.shape}') if debug else None\n",
    "        all_key_query_values = self.all_key_query_values(x)\n",
    "        print(f'all_key_query_values.shape: {all_key_query_values.shape}') if debug else None\n",
    "        print([k.shape for k in all_key_query_values.split(self.head_size*3, dim=-1)]) if debug else None\n",
    "\n",
    "        out = torch.cat([self.attention_head(y) for y in all_key_query_values.split(self.head_size*3, dim=-1)], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.combined_dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        # self.attn = MultiHeadAttention(n_head, head_size)\n",
    "        self.attn = CausalSelfAttention()\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Only normalized over embedding. Batch and time act as batch dimension.\n",
    "        x = x + self.attn(self.ln1(x)) \n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "torch.manual_seed(3)\n",
    "model = GPTLanguageModel()\n",
    "save_path = 'models/model_weights_13_1-2572.pth'\n",
    "model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "model = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(f'{sum(p.numel() for p in model.parameters())/1e6:.2f} Million parameters')\n",
    "\n",
    "# torch.manual_seed(3)\n",
    "# tor\n",
    "import random\n",
    "seed = random.randint(0, 1000)\n",
    "seed = 3\n",
    "print(f'seed: {seed}')\n",
    "torch.manual_seed(seed)\n",
    "model.eval()\n",
    "context = torch.randint(0, vocab_size, (1, 2), device=device)\n",
    "with open('generated.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))\n",
    "###\n",
    "\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    total_params += num_params\n",
    "    if num_params > 10000:\n",
    "        print(f'{name}: {num_params:,} parameters')\n",
    "\n",
    "print(f'\\nTotal: {total_params:,} parameters ({total_params/1e6:.2f}M)')\n",
    "\n",
    "### training loop ###\n",
    "\n",
    "import os\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "minimum_loss = 100\n",
    "\n",
    "model.train()\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < minimum_loss:\n",
    "            minimum_loss = losses['val']\n",
    "            print(f'----- new minimum loss: {minimum_loss} -----')\n",
    "            os.makedirs('models', exist_ok=True)\n",
    "            folder_size = len(os.listdir('models'))+1\n",
    "            formatted_loss = f'{minimum_loss:.4f}'.replace('.', '-')\n",
    "            save_path = f'models/model_weights_{folder_size}_{formatted_loss}.pth'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"save_path = '{save_path}'\")\n",
    "\n",
    "            model.eval()\n",
    "            context = torch.randint(0, vocab_size, (1, 2), device=device)\n",
    "            print(decode(model.generate(context, max_new_tokens=200)[0].tolist()))\n",
    "            model.train()\n",
    "            print('-----')\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "# step 2499: train loss 1.1224, val loss 1.3344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{sum(p.numel() for p in model.parameters())/1e6:.2f} Million parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding_table.weight: 32,256 parameters\n",
      "position_embedding_table.weight: 98,304 parameters\n",
      "blocks.0.attn.c_attn.weight: 442,368 parameters\n",
      "blocks.0.attn.c_proj.weight: 147,456 parameters\n",
      "blocks.0.ffwd.net.0.weight: 589,824 parameters\n",
      "blocks.0.ffwd.net.2.weight: 589,824 parameters\n",
      "blocks.1.attn.c_attn.weight: 442,368 parameters\n",
      "blocks.1.attn.c_proj.weight: 147,456 parameters\n",
      "blocks.1.ffwd.net.0.weight: 589,824 parameters\n",
      "blocks.1.ffwd.net.2.weight: 589,824 parameters\n",
      "blocks.2.attn.c_attn.weight: 442,368 parameters\n",
      "blocks.2.attn.c_proj.weight: 147,456 parameters\n",
      "blocks.2.ffwd.net.0.weight: 589,824 parameters\n",
      "blocks.2.ffwd.net.2.weight: 589,824 parameters\n",
      "blocks.3.attn.c_attn.weight: 442,368 parameters\n",
      "blocks.3.attn.c_proj.weight: 147,456 parameters\n",
      "blocks.3.ffwd.net.0.weight: 589,824 parameters\n",
      "blocks.3.ffwd.net.2.weight: 589,824 parameters\n",
      "blocks.4.attn.c_attn.weight: 442,368 parameters\n",
      "blocks.4.attn.c_proj.weight: 147,456 parameters\n",
      "blocks.4.ffwd.net.0.weight: 589,824 parameters\n",
      "blocks.4.ffwd.net.2.weight: 589,824 parameters\n",
      "blocks.5.attn.c_attn.weight: 442,368 parameters\n",
      "blocks.5.attn.c_proj.weight: 147,456 parameters\n",
      "blocks.5.ffwd.net.0.weight: 589,824 parameters\n",
      "blocks.5.ffwd.net.2.weight: 589,824 parameters\n",
      "lm_head.weight: 32,256 parameters\n",
      "\n",
      "Total: 10,810,452 parameters (10.81M)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4643, val loss 4.4655\n",
      "----- new minimum loss: 4.465526580810547 -----\n",
      "save_path = 'models/model_weights_9_4-4655.pth'\n",
      "\\JkS84#i'w”/hKt,GXNX ,Oa\"\\8*tpSWQ9AtjjN81SIR(kjQp5 \\rQ'-p*Q7i*\\*?k\n",
      "L3Bm5qkCk3)C%xCzABouQI,8\"j—au’8hXr)DK8'# 8KIz !gf 0PK—5Fp‘a—p4,y’8)Kk?k:Z3(rKkmyk‘cX8i’)z4 I”y”8QEx*aw5OkK k\\—”-Q ioW87:XqD)/vDI0;*oO—?\n",
      "-----\n",
      "step 500: train loss 1.7915, val loss 1.8085\n",
      "----- new minimum loss: 1.8084900379180908 -----\n",
      "save_path = 'models/model_weights_10_1-8085.pth'\n",
      "Jk, the have Tay But there lim ago of rem-ounders \n",
      "comparsis. He is besuiblanight it a chancelf the hand thunds and coperry vace on hour wouth \n",
      "he stroir. \n",
      "\n",
      "He was of his compen ther. Grangw she vermsin\n",
      "-----\n",
      "step 1000: train loss 1.3662, val loss 1.4144\n",
      "----- new minimum loss: 1.4143580198287964 -----\n",
      "save_path = 'models/model_weights_11_1-4144.pth'\n",
      "eget him to perhaps with a weak. If the hand to keept the time \n",
      "plase it! We may benched to come the nergy of ten pieces of the sGalaxy-gadmoner backed capital \n",
      "up it. We can't perful; and thought the S\n",
      "-----\n",
      "step 1500: train loss 1.2322, val loss 1.3037\n",
      "----- new minimum loss: 1.30369234085083 -----\n",
      "save_path = 'models/model_weights_12_1-3037.pth'\n",
      "vwards will expange the Convention. I real the \n",
      "man unconquerormental arms ago.\" \n",
      "\n",
      "Bayta said's form his hands recording spleen strong with the fevolt. It was followed, \"You \n",
      "existed that?\" \n",
      "\n",
      "The moment\n",
      "-----\n",
      "step 2000: train loss 1.1489, val loss 1.2572\n",
      "----- new minimum loss: 1.2571600675582886 -----\n",
      "save_path = 'models/model_weights_13_1-2572.pth'\n",
      "s; was - and \n",
      "not meet her pictuously start into his weaker. \n",
      "\n",
      "\"No,\" said Pirenne, carefully, \"and well, may I concil stive impernation with you. I need unanother schoil, \n",
      "I could really those barbarous\n",
      "-----\n",
      "step 2500: train loss 1.0912, val loss 1.2351\n",
      "----- new minimum loss: 1.2350537776947021 -----\n",
      "save_path = 'models/model_weights_14_1-2351.pth'\n",
      ":it has not \n",
      "the preminent destruction for the metal intentions to it, which stinking and there are the mental \n",
      "region of conspiration unanimous of structures and blown at suggestion at the irror own po\n",
      "-----\n",
      "step 3000: train loss 1.0435, val loss 1.2279\n",
      "----- new minimum loss: 1.2278869152069092 -----\n",
      "save_path = 'models/model_weights_15_1-2279.pth'\n",
      "'quite so new blast. \"Good. Why do you follow you \n",
      "me?\" \n",
      "\n",
      "Forced them, rotten softly, and your pockets, \"The second master to the emotional science, and I felt \n",
      "at that sake expect, and I'm sure what yo\n",
      "-----\n",
      "step 3500: train loss 0.9977, val loss 1.2280\n",
      "step 4000: train loss 0.9576, val loss 1.2292\n",
      "step 4500: train loss 0.9175, val loss 1.2349\n",
      "step 4999: train loss 0.8733, val loss 1.2491\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
